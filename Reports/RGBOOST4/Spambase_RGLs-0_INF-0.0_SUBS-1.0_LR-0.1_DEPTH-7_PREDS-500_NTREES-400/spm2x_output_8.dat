
 The "USE "../Data/Classification/Spambase/SAMPLES4/data" command: 00:00:00

 Model (target and predictors) reset: SPAM_OR_NOTSPAM

 The KEEP list has 57 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 2301
 Records Kept in Learning sample: 2301

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 1150
 Records Kept in Test sample: 1150

    Discrete         N Levels
    Variable         in Model
 ----------------------------
 SPAM_OR_NOTSPAM            2

 ======================
 Target Frequency Table
 ======================

 Variable: SPAM_OR_NOTSPAM
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         1394  60.58              1394.00  60.58
                 T         (697  60.61)             (697.00  60.61)
 1               L          907  39.42               907.00  39.42
                 T         (453  39.39)             (453.00  39.39)
 -----------------------------------------------------------------
 Totals
 0                         2091  60.59              2091.00  60.59
 1                         1360  39.41              1360.00  39.41
 -----------------------------------------------------------------
 Total                     3451                     3451.00
 Total Learn               2301                     2301.00
 Total Test                1150                     1150.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.59504      0.60142      0.39418      0.39391      1.00000 |                                               *
      2       0.53221      0.54292      0.10691      0.11652      1.00000 |                                          *
      3       0.47975      0.49518      0.06171      0.09739      1.00000 |                                       *
      4       0.43546      0.45533      0.04954      0.08783      1.00000 |                                   *
      5       0.39758      0.42088      0.04520      0.07478      1.00000 |                                 *
      6       0.36451      0.39157      0.04216      0.07043      1.00000 |                              *
      7       0.33656      0.36661      0.03955      0.06957      1.00000 |                            *
      8       0.31146      0.34558      0.03781      0.06957      1.00000 |                           *
      9       0.28925      0.32729      0.03824      0.06870      1.00000 |                         *
     10       0.26862      0.30890      0.03477      0.06348      1.00000 |                        *
     11       0.25102      0.29372      0.03129      0.06522      1.00000 |                      *
     12       0.23439      0.27877      0.03042      0.06435      1.00000 |                     *
     13       0.21979      0.26680      0.02955      0.06087      1.00000 |                    *
     14       0.20632      0.25650      0.02955      0.06348      1.00000 |                   *
     15       0.19484      0.24596      0.02868      0.06174      1.00000 |                   *
     16       0.18224      0.23567      0.02781      0.06000      1.00000 |                  *
     17       0.17126      0.22715      0.02694      0.05739      1.00000 |                 *
     18       0.16241      0.22098      0.02781      0.06000      1.00000 |                 *
     19       0.15473      0.21406      0.02694      0.05826      1.00000 |                *
     20       0.14608      0.20784      0.02477      0.05652      1.00000 |                *
     30       0.09274      0.17157      0.01912      0.05826      1.00000 |             *
     40       0.06620      0.15302      0.01608      0.05826      1.00000 |           *
     50       0.05162      0.14433      0.01000      0.05391      1.00000 |           *
     60       0.04363      0.13856      0.00913      0.05130      1.00000 |          *
     70       0.03691      0.13609      0.00695      0.05130      1.00000 |          *
     80       0.03043      0.13249      0.00304      0.05043      1.00000 |          *
     90       0.02650      0.12990      0.00261      0.04609      1.00000 |         *
    100       0.02250      0.12943      0.00130      0.04522      1.00000 |         *
    110       0.02008      0.12912      0.00130      0.04696      1.00000 |         *
    120       0.01783      0.12855      0.00130      0.04435      1.00000 |         *
    130       0.01559      0.13026      0.00087      0.04522      1.00000 |         *
    140       0.01358      0.13179      0.00087      0.04435      1.00000 |          *
    150       0.01157      0.13235      0.00043      0.04435      1.00000 |          *
    160       0.01022      0.13379      0.00043      0.04435      1.00000 |          *
    170       0.00888      0.13547      0.00043      0.04522      1.00000 |          *
    180       0.00776      0.13741      0.00043      0.04522      1.00000 |          *
    190       0.00692      0.13899      0.00043      0.04435      1.00000 |          *
    200       0.00584      0.14017      0.00043      0.04261      1.00000 |          *
    210       0.00480      0.14289      0.00000      0.04087      1.00000 |          *
    220       0.00411      0.14654      0.00000      0.04087      1.00000 |           *
    230       0.00334      0.14913      0.00000      0.04087      1.00000 |           *
    240       0.00284      0.15263      0.00000      0.04174      1.00000 |           *
    250       0.00241      0.15621      0.00000      0.04261      1.00000 |           *
    260       0.00220      0.15805      0.00000      0.04348      1.00000 |            *
    270       0.00182      0.16057      0.00000      0.04348      1.00000 |            *
    280       0.00160      0.16384      0.00000      0.04435      1.00000 |            *
    290       0.00130      0.16862      0.00000      0.04348      1.00000 |            *
    300       0.00114      0.17159      0.00000      0.04174      1.00000 |             *
    310       0.00089      0.17727      0.00000      0.04348      1.00000 |             *
    320       0.00077      0.18003      0.00000      0.04261      1.00000 |             *
    330       0.00064      0.18393      0.00000      0.04348      1.00000 |              *
    340       0.00057      0.18709      0.00000      0.04348      1.00000 |              *
    350       0.00049      0.19056      0.00000      0.04261      1.00000 |              *
    360       0.00041      0.19445      0.00000      0.04174      1.00000 |               *
    370       0.00033      0.19735      0.00000      0.04261      1.00000 |               *
    380       0.00028      0.20090      0.00000      0.04174      1.00000 |               *
    390       0.00025      0.20485      0.00000      0.04174      1.00000 |               *
    400       0.00022      0.20761      0.00000      0.04087      1.00000 |                *

 Core TN model building:          1.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    391      1    400      1      7      4.35      3.57  CAPITAL_RUN_LENGTH_AVERAGE
    378      1    400      2      7      4.44      3.36  CAPITAL_RUN_LENGTH_TOTAL
    361      1    399      1      7      4.16      3.47  CHAR_FREQ_EXCLAMATION
    358      1    400      1      7      4.93      2.75  WORD_FREQ_YOU
    351      1    400      2      7      4.82      2.79  CAPITAL_RUN_LENGTH_LONGEST
    328      2    400      1      7      4.54      2.84  WORD_FREQ_YOUR
    320      1    399      1      7      4.86      2.52  CHAR_FREQ_BRACKET
    315      2    399      1      7      4.74      2.57  WORD_FREQ_WILL
    278      1    400      1      7      4.54      2.41  WORD_FREQ_HP
    271      1    400      1      7      5.10      1.97  WORD_FREQ_OUR
    253      1    400      1      7      4.83      2.01  CHAR_FREQ_DOLLAR
    246      2    400      1      7      5.02      1.83  WORD_FREQ_RE
    235      1    398      1      7      4.89      1.83  WORD_FREQ_FREE
    224      1    400      2      7      5.29      1.52  WORD_FREQ_MAIL
    218      1    397      1      7      4.50      1.91  WORD_FREQ_REMOVE
    209      1    399      2      7      5.00      1.57  WORD_FREQ_BUSINESS
    197      1    397      1      7      5.21      1.38  WORD_FREQ_ALL
    189      1    400      1      7      4.49      1.66  WORD_FREQ_EDU
    187      2    398      1      7      5.06      1.37  CHAR_FREQ_SEMICOLON
    186      1    396      1      7      3.92      1.90  WORD_FREQ_GEORGE
    180     19    398      1      7      3.63      1.97  WORD_FREQ_MAKE
    179      3    396      1      7      5.13      1.29  WORD_FREQ_EMAIL
    161      1    399      2      7      5.42      1.04  WORD_FREQ_INTERNET
    154      1    399      1      7      5.45      0.98  WORD_FREQ_1999
    149      2    399      2      7      4.21      1.41  WORD_FREQ_ORDER
    134      4    399      2      7      5.25      0.92  WORD_FREQ_RECEIVE
    128      1    395      1      7      3.49      1.44  WORD_FREQ_MEETING
    125      3    399      1      7      5.15      0.89  WORD_FREQ_OVER
    120      1    397      1      7      3.98      1.21  WORD_FREQ_MONEY
    119      1    399      1      7      5.56      0.73  WORD_FREQ_ADDRESS
    117      1    399      1      7      4.64      0.98  WORD_FREQ_000
    113      3    399      2      7      5.58      0.68  CHAR_FREQ_HASH
    111      7    400      1      7      4.68      0.92  WORD_FREQ_TECHNOLOGY
    110      2    400      1      7      5.25      0.76  WORD_FREQ_HPL
     99     10    399      1      7      4.57      0.85  WORD_FREQ_650
     97      2    400      2      7      4.37      0.88  WORD_FREQ_REPORT
     92      4    400      1      7      5.12      0.66  WORD_FREQ_DATA
     91     16    395      3      7      5.65      0.54  CHAR_FREQ_BOXBRACKET
     88      5    399      3      7      6.03      0.43  WORD_FREQ_PEOPLE
     73     18    399      1      7      5.15      0.52  WORD_FREQ_PROJECT
     61      1    399      1      7      5.43      0.39  WORD_FREQ_PM
     59      4    400      1      7      4.90      0.46  WORD_FREQ_85
     59     31    388      1      7      4.14      0.57  WORD_FREQ_CONFERENCE
     53     29    398      2      7      5.09      0.39  WORD_FREQ_DIRECT
     52      9    397      2      7      5.17      0.37  WORD_FREQ_ORIGINAL
     51     11    399      1      7      4.84      0.40  WORD_FREQ_CREDIT
     46      3    367      1      7      3.83      0.48  WORD_FREQ_LAB
     44     10    394      2      7      4.95      0.34  WORD_FREQ_LABS
     38      2    381      4      7      5.71      0.22  WORD_FREQ_ADDRESSES
     28     48    392      2      7      4.21      0.27  WORD_FREQ_FONT
     26     52    387      1      7      2.69      0.35  WORD_FREQ_CS
     21     18    363      3      7      6.05      0.10  WORD_FREQ_TELNET
     19     42    338      2      7      4.95      0.15  WORD_FREQ_PARTS
      7      9    362      1      7      3.00      0.09  WORD_FREQ_3D
      5    113    327      6      7      6.20      0.02  WORD_FREQ_TABLE
      0      0      0      0      0      0.00      0.00  WORD_FREQ_415
      0      0      0      0      0      0.00      0.00  WORD_FREQ_857

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:    18 terminal nodes
    Largest :    92 terminal nodes
    Average :     42.35250 terminal nodes

 Reconciling 2301 Learn sample scores across 5 selected models,
 the largest having 234 trees, to compute gains and PS tables.

 Reconciling 1150 Test sample scores across 5 selected models,
 the largest having 234 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 120

 Target: SPAM_OR_NOTSPAM
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     2301      2301.00
 N Test  Obs:     1150      1150.00
 Learn Rate :    0.1000000

 Storage requirements: 33482 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL          120      0.12855
                  ROC          234      0.98919
                 Lift           19      2.53863
              KS-stat          232      0.91932
          Class.Error          212      0.04000

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.59504     0.60142     0.96259     0.92663     2.53693     2.31788     0.86926     0.79603     0.39418     0.39391
     10     0.26862     0.30890     0.98757     0.96938     2.53693     2.51656     0.92512     0.86358     0.03477     0.06348
     19     0.15473     0.21406     0.99550     0.97877     2.53693     2.53863     0.94376     0.87716     0.02694     0.05826
     20     0.14608     0.20784     0.99594     0.97952     2.53693     2.53863     0.94586     0.87936     0.02477     0.05652
     30     0.09274     0.17157     0.99794     0.98324     2.53693     2.53863     0.96614     0.87981     0.01912     0.05826
     40     0.06620     0.15302     0.99875     0.98566     2.53693     2.53863     0.97232     0.88356     0.01608     0.05826
     50     0.05162     0.14433     0.99919     0.98580     2.53693     2.53863     0.98262     0.89073     0.01000     0.05391
     60     0.04363     0.13856     0.99938     0.98685     2.53693     2.53863     0.98616     0.89736     0.00913     0.05130
     70     0.03691     0.13609     0.99959     0.98667     2.53693     2.53863     0.98941     0.89736     0.00695     0.05130
     80     0.03043     0.13249     0.99975     0.98741     2.53693     2.53863     0.99377     0.90288     0.00304     0.05043
     90     0.02650     0.12990     0.99980     0.98799     2.53693     2.53863     0.99559     0.90640     0.00261     0.04609
    100     0.02250     0.12943     0.99990     0.98827     2.53693     2.53863     0.99708     0.91038     0.00130     0.04522
    110     0.02008     0.12912     0.99991     0.98850     2.53693     2.53863     0.99779     0.90828     0.00130     0.04696
    120     0.01783     0.12855     0.99994     0.98879     2.53693     2.53863     0.99890     0.90906     0.00130     0.04435
    130     0.01559     0.13026     0.99996     0.98873     2.53693     2.53863     0.99890     0.91181     0.00087     0.04522
    140     0.01358     0.13179     0.99999     0.98868     2.53693     2.53863     0.99890     0.91192     0.00087     0.04435
    150     0.01157     0.13235     1.00000     0.98874     2.53693     2.53863     1.00000     0.91336     0.00043     0.04435
    160     0.01022     0.13379     1.00000     0.98874     2.53693     2.53863     1.00000     0.91435     0.00043     0.04435
    170     0.00888     0.13547     1.00000     0.98879     2.53693     2.53863     1.00000     0.91435     0.00043     0.04522
    180     0.00776     0.13741     1.00000     0.98893     2.53693     2.53863     1.00000     0.91413     0.00043     0.04522
    190     0.00692     0.13899     1.00000     0.98892     2.53693     2.53863     1.00000     0.91645     0.00043     0.04435
    200     0.00584     0.14017     1.00000     0.98905     2.53693     2.53863     1.00000     0.91789     0.00043     0.04261
    205     0.00533     0.14119     1.00000     0.98910     2.53693     2.53863     1.00000     0.91645     0.00000     0.04261
    210     0.00480     0.14289     1.00000     0.98913     2.53693     2.53863     1.00000     0.91700     0.00000     0.04087
    212     0.00468     0.14383     1.00000     0.98909     2.53693     2.53863     1.00000     0.91700     0.00000     0.04000
    220     0.00411     0.14654     1.00000     0.98895     2.53693     2.53863     1.00000     0.91645     0.00000     0.04087
    230     0.00334     0.14913     1.00000     0.98915     2.53693     2.53863     1.00000     0.91645     0.00000     0.04087
    232     0.00321     0.14988     1.00000     0.98913     2.53693     2.53863     1.00000     0.91932     0.00000     0.04174
    234     0.00308     0.15033     1.00000     0.98919     2.53693     2.53863     1.00000     0.91789     0.00000     0.04087
    240     0.00284     0.15263     1.00000     0.98906     2.53693     2.53863     1.00000     0.91645     0.00000     0.04174
    250     0.00241     0.15621     1.00000     0.98898     2.53693     2.53863     1.00000     0.91789     0.00000     0.04261
    260     0.00220     0.15805     1.00000     0.98903     2.53693     2.53863     1.00000     0.91711     0.00000     0.04348
    270     0.00182     0.16057     1.00000     0.98896     2.53693     2.53863     1.00000     0.91568     0.00000     0.04348
    280     0.00160     0.16384     1.00000     0.98890     2.53693     2.53863     1.00000     0.91777     0.00000     0.04435
    290     0.00130     0.16862     1.00000     0.98873     2.53693     2.53863     1.00000     0.91777     0.00000     0.04348
    300     0.00114     0.17159     1.00000     0.98874     2.53693     2.53863     1.00000     0.91700     0.00000     0.04174
    310     0.00089     0.17727     1.00000     0.98860     2.53693     2.53863     1.00000     0.91789     0.00000     0.04348
    320     0.00077     0.18003     1.00000     0.98876     2.53693     2.53863     1.00000     0.91557     0.00000     0.04261
    330     0.00064     0.18393     1.00000     0.98860     2.53693     2.53863     1.00000     0.91568     0.00000     0.04348
    340     0.00057     0.18709     1.00000     0.98858     2.53693     2.53863     1.00000     0.91557     0.00000     0.04348
    350     0.00049     0.19056     1.00000     0.98838     2.53693     2.53863     1.00000     0.91645     0.00000     0.04261
    360     0.00041     0.19445     1.00000     0.98834     2.53693     2.53863     1.00000     0.91645     0.00000     0.04174
    370     0.00033     0.19735     1.00000     0.98833     2.53693     2.53863     1.00000     0.91789     0.00000     0.04261
    380     0.00028     0.20090     1.00000     0.98834     2.53693     2.53863     1.00000     0.91623     0.00000     0.04174
    390     0.00025     0.20485     1.00000     0.98822     2.53693     2.53863     1.00000     0.91623     0.00000     0.04174
    400     0.00022     0.20761     1.00000     0.98823     2.53693     2.53863     1.00000     0.91535     0.00000     0.04087


 ==========================================
 Variable Importance for the 120-tree Model
 ==========================================

                                     Abs     Rel

 CHAR_FREQ_EXCLAMATION         100.00000  100.00 |***********|
 WORD_FREQ_REMOVE               74.49808   74.50 |********   |
 CHAR_FREQ_DOLLAR               74.46165   74.46 |********   |
 WORD_FREQ_HP                   50.33803   50.34 |******     |
 WORD_FREQ_YOUR                 46.17295   46.17 |******     |
 CAPITAL_RUN_LENGTH_AVERAGE     44.95269   44.95 |*****      |
 CAPITAL_RUN_LENGTH_TOTAL       38.72124   38.72 |*****      |
 WORD_FREQ_GEORGE               37.42794   37.43 |*****      |
 WORD_FREQ_FREE                 35.58335   35.58 |*****      |
 WORD_FREQ_EDU                  32.24086   32.24 |****       |
 WORD_FREQ_OUR                  29.92023   29.92 |****       |
 CAPITAL_RUN_LENGTH_LONGEST     27.64631   27.65 |****       |
 WORD_FREQ_MONEY                24.62595   24.63 |***        |
 WORD_FREQ_MEETING              23.72906   23.73 |***        |
 WORD_FREQ_YOU                  22.18887   22.19 |***        |
 WORD_FREQ_WILL                 22.17032   22.17 |***        |
 WORD_FREQ_BUSINESS             22.11286   22.11 |***        |
 WORD_FREQ_RE                   21.86330   21.86 |***        |
 WORD_FREQ_INTERNET             20.32727   20.33 |***        |
 WORD_FREQ_000                  20.29652   20.30 |***        |
 WORD_FREQ_1999                 20.02194   20.02 |***        |
 CHAR_FREQ_BRACKET              19.22738   19.23 |***        |
 WORD_FREQ_MAIL                 16.96674   16.97 |***        |
 CHAR_FREQ_SEMICOLON            15.55281   15.55 |***        |
 WORD_FREQ_650                  14.04597   14.05 |**         |
 WORD_FREQ_HPL                  14.04570   14.05 |**         |
 WORD_FREQ_RECEIVE              13.45873   13.46 |**         |
 WORD_FREQ_EMAIL                13.23773   13.24 |**         |
 WORD_FREQ_OVER                 13.17425   13.17 |**         |
 WORD_FREQ_CREDIT               12.47297   12.47 |**         |
 WORD_FREQ_PM                   12.19450   12.19 |**         |
 WORD_FREQ_TECHNOLOGY           10.65115   10.65 |**         |
 WORD_FREQ_DATA                  9.50548    9.51 |**         |
 CHAR_FREQ_HASH                  9.47038    9.47 |**         |
 WORD_FREQ_PROJECT               9.25528    9.26 |**         |
 WORD_FREQ_ALL                   9.00878    9.01 |**         |
 WORD_FREQ_ADDRESS               7.27855    7.28 |**         |
 WORD_FREQ_REPORT                7.21212    7.21 |**         |
 WORD_FREQ_MAKE                  7.18720    7.19 |**         |
 WORD_FREQ_ORDER                 7.13842    7.14 |**         |
 WORD_FREQ_LAB                   7.12094    7.12 |**         |
 WORD_FREQ_ADDRESSES             6.45719    6.46 |**         |
 WORD_FREQ_FONT                  5.77094    5.77 |**         |
 WORD_FREQ_PEOPLE                5.70093    5.70 |**         |
 WORD_FREQ_CS                    4.55164    4.55 |*          |
 WORD_FREQ_CONFERENCE            4.02316    4.02 |*          |
 WORD_FREQ_LABS                  3.78206    3.78 |*          |
 WORD_FREQ_ORIGINAL              3.65310    3.65 |*          |
 WORD_FREQ_TELNET                3.17816    3.18 |*          |
 CHAR_FREQ_BOXBRACKET            3.16164    3.16 |*          |
 WORD_FREQ_85                    2.86426    2.86 |*          |
 WORD_FREQ_DIRECT                1.68419    1.68 |*          |
 WORD_FREQ_PARTS                 1.40815    1.41 |*          |
 WORD_FREQ_3D                    0.72102    0.72 |*          |
 WORD_FREQ_TABLE                 0.00009    0.00 |*          |


 Learn Sample Misclassification by Target Class
 For The 120-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 1394.00      1394.00         0.00       0.0000
 1                  907.00       904.00         3.00       0.0033


 Test Sample Misclassification by Target Class
 For The 120-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                  697.00       675.00        22.00       0.0316
 1                  453.00       424.00        29.00       0.0640

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                1.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               1.000 sec ( 0.00 hrs, 100.00%)
    Core model:         1.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             0.000 sec ( 0.00 hrs,   0.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000000 hrs  0.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/treenet2x_model.grv: 1.5 MB, 75% compression

 Grove file created containing:
      1 TreeNet

