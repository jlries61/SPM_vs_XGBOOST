
 The "USE "../Data/Classification/Spambase/SAMPLES4/data" command: 00:00:00

 Model (target and predictors) reset: SPAM_OR_NOTSPAM

 The KEEP list has 57 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 2301
 Records Kept in Learning sample: 2301

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 1150
 Records Kept in Test sample: 1150

    Discrete         N Levels
    Variable         in Model
 ----------------------------
 SPAM_OR_NOTSPAM            2

 ======================
 Target Frequency Table
 ======================

 Variable: SPAM_OR_NOTSPAM
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         1394  60.58              1394.00  60.58
                 T         (697  60.61)             (697.00  60.61)
 1               L          907  39.42               907.00  39.42
                 T         (453  39.39)             (453.00  39.39)
 -----------------------------------------------------------------
 Totals
 0                         2091  60.59              2091.00  60.59
 1                         1360  39.41              1360.00  39.41
 -----------------------------------------------------------------
 Total                     3451                     3451.00
 Total Learn               2301                     2301.00
 Total Test                1150                     1150.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.59776      0.60291      0.39418      0.39391      1.00000 |                                               *
      2       0.53906      0.54935      0.11864      0.14087      1.00000 |                                           *
      3       0.49032      0.50522      0.07953      0.12174      1.00000 |                                       *
      4       0.44964      0.46916      0.07345      0.11304      1.00000 |                                    *
      5       0.41438      0.43698      0.06258      0.10261      1.00000 |                                  *
      6       0.38284      0.40778      0.06345      0.09826      1.00000 |                               *
      7       0.35684      0.38502      0.06302      0.09739      1.00000 |                              *
      8       0.33350      0.36411      0.05954      0.09826      1.00000 |                            *
      9       0.31184      0.34516      0.05693      0.09304      1.00000 |                          *
     10       0.29092      0.32688      0.05476      0.08957      1.00000 |                         *
     11       0.27374      0.31155      0.05519      0.09043      1.00000 |                        *
     12       0.25819      0.29904      0.04998      0.08522      1.00000 |                       *
     13       0.24456      0.28677      0.04954      0.08348      1.00000 |                      *
     14       0.23202      0.27709      0.04607      0.07826      1.00000 |                     *
     15       0.21997      0.26551      0.04433      0.07826      1.00000 |                    *
     16       0.20932      0.25604      0.04259      0.07391      1.00000 |                   *
     17       0.19990      0.24791      0.04129      0.07391      1.00000 |                   *
     18       0.18977      0.23876      0.03955      0.07304      1.00000 |                  *
     19       0.18054      0.23045      0.03781      0.06870      1.00000 |                 *
     20       0.17229      0.22340      0.03651      0.06522      1.00000 |                 *
     30       0.11782      0.17854      0.03129      0.06174      1.00000 |             *
     40       0.08743      0.15595      0.02086      0.05130      1.00000 |           *
     50       0.07228      0.14639      0.01738      0.05130      1.00000 |           *
     60       0.06279      0.14098      0.01434      0.04957      1.00000 |          *
     70       0.05452      0.13765      0.01173      0.04696      1.00000 |          *
     80       0.05008      0.13481      0.01130      0.04696      1.00000 |          *
     90       0.04339      0.13308      0.01000      0.04609      1.00000 |          *
    100       0.03723      0.13230      0.00652      0.04522      1.00000 |          *
    110       0.03306      0.13074      0.00522      0.04522      1.00000 |         *
    120       0.02922      0.13067      0.00391      0.04435      1.00000 |         *
    130       0.02515      0.12944      0.00304      0.04348      1.00000 |         *
    140       0.02202      0.12806      0.00261      0.04087      1.00000 |         *
    150       0.01942      0.12861      0.00174      0.04261      1.00000 |         *
    160       0.01733      0.12841      0.00130      0.04174      1.00000 |         *
    170       0.01575      0.12851      0.00130      0.04174      1.00000 |         *
    180       0.01388      0.12902      0.00087      0.04174      1.00000 |         *
    190       0.01269      0.12918      0.00087      0.04348      1.00000 |         *
    200       0.01132      0.13060      0.00087      0.04087      1.00000 |         *
    210       0.01002      0.13267      0.00087      0.04348      1.00000 |          *
    220       0.00910      0.13395      0.00087      0.04087      1.00000 |          *
    230       0.00815      0.13465      0.00087      0.04087      1.00000 |          *
    240       0.00744      0.13635      0.00087      0.04174      1.00000 |          *
    250       0.00670      0.13761      0.00087      0.04174      1.00000 |          *
    260       0.00600      0.13883      0.00087      0.04174      1.00000 |          *
    270       0.00550      0.14058      0.00043      0.04174      1.00000 |          *
    280       0.00504      0.14210      0.00043      0.04261      1.00000 |          *
    290       0.00461      0.14373      0.00043      0.04348      1.00000 |          *
    300       0.00422      0.14539      0.00043      0.04348      1.00000 |           *
    310       0.00392      0.14743      0.00043      0.04435      1.00000 |           *
    320       0.00356      0.14924      0.00043      0.04435      1.00000 |           *
    330       0.00324      0.15034      0.00043      0.04435      1.00000 |           *
    340       0.00297      0.15203      0.00043      0.04435      1.00000 |           *
    350       0.00277      0.15343      0.00043      0.04435      1.00000 |           *
    360       0.00258      0.15465      0.00043      0.04348      1.00000 |           *
    370       0.00241      0.15644      0.00043      0.04348      1.00000 |           *
    380       0.00225      0.15779      0.00043      0.04261      1.00000 |            *
    390       0.00208      0.15876      0.00043      0.04261      1.00000 |            *
    400       0.00192      0.16047      0.00043      0.04261      1.00000 |            *

 Core TN model building:          1.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    350      1    400      1      7      4.24      3.29  CAPITAL_RUN_LENGTH_TOTAL
    343      1    400      1      7      4.62      2.90  WORD_FREQ_YOU
    334      1    400      1      7      4.41      3.00  CAPITAL_RUN_LENGTH_AVERAGE
    292      1    400      1      7      4.14      2.82  CHAR_FREQ_EXCLAMATION
    291      1    398      1      7      4.59      2.48  CAPITAL_RUN_LENGTH_LONGEST
    266      1    400      1      7      4.36      2.42  WORD_FREQ_YOUR
    209      1    398      1      7      4.90      1.62  CHAR_FREQ_BRACKET
    188      2    400      1      7      4.88      1.47  WORD_FREQ_WILL
    183      1    399      1      7      4.04      1.81  WORD_FREQ_HP
    173      1    399      1      7      4.26      1.62  WORD_FREQ_FREE
    165      1    398      1      7      5.05      1.22  WORD_FREQ_OUR
    158      3    398      1      7      4.30      1.46  WORD_FREQ_RE
    152      1    397      1      7      4.38      1.38  CHAR_FREQ_DOLLAR
    140      1    400      2      7      5.24      0.97  WORD_FREQ_MAIL
    130      4    385      1      7      4.38      1.18  WORD_FREQ_GEORGE
    127      1    396      1      7      5.00      0.95  WORD_FREQ_1999
    103      6    397      2      7      5.29      0.70  WORD_FREQ_BUSINESS
    101      1    397      1      7      3.20      1.21  WORD_FREQ_REMOVE
     97      2    398      1      7      5.43      0.62  WORD_FREQ_ALL
     93      1    396      1      7      3.53      1.04  WORD_FREQ_EDU
     88      3    398      2      7      5.09      0.64  WORD_FREQ_EMAIL
     88     23    397      1      7      4.91      0.68  WORD_FREQ_MAKE
     87      1    396      2      7      5.37      0.57  WORD_FREQ_RECEIVE
     76      4    397      1      7      3.64      0.83  CHAR_FREQ_SEMICOLON
     75     16    396      2      7      3.77      0.79  WORD_FREQ_650
     70      1    394      1      7      5.19      0.49  WORD_FREQ_HPL
     68      1    398      1      7      3.85      0.71  WORD_FREQ_INTERNET
     64     14    396      1      7      2.80      0.83  WORD_FREQ_MEETING
     63      6    398      1      7      3.49      0.71  WORD_FREQ_PEOPLE
     62      9    394      1      7      3.69      0.67  WORD_FREQ_DATA
     53     13    379      1      7      4.89      0.41  WORD_FREQ_OVER
     50     12    389      1      7      5.54      0.31  WORD_FREQ_ADDRESS
     48      1    397      1      7      4.38      0.44  WORD_FREQ_000
     47     19    388      1      7      4.04      0.47  WORD_FREQ_REPORT
     40      8    381      1      7      2.80      0.52  WORD_FREQ_MONEY
     38     18    393      2      7      4.71      0.31  CHAR_FREQ_BOXBRACKET
     32     26    393      1      6      3.09      0.39  WORD_FREQ_CONFERENCE
     30     15    364      1      7      3.63      0.33  WORD_FREQ_PROJECT
     27     10    389      1      7      5.19      0.19  WORD_FREQ_TECHNOLOGY
     27     24    392      2      7      4.96      0.21  CHAR_FREQ_HASH
     26     10    361      1      7      3.54      0.29  WORD_FREQ_PM
     26     13    379      2      7      3.00      0.33  WORD_FREQ_FONT
     26     10    372      1      7      5.23      0.18  WORD_FREQ_ORDER
     25     49    384      1      5      1.76      0.39  WORD_FREQ_3D
     23      9    394      3      7      5.00      0.17  WORD_FREQ_85
     17     35    396      2      7      5.65      0.10  WORD_FREQ_LAB
     17     23    286      3      7      3.71      0.18  WORD_FREQ_ORIGINAL
     16     18    361      2      7      4.31      0.15  WORD_FREQ_LABS
     16      5    372      2      7      4.25      0.15  WORD_FREQ_DIRECT
     16      3    379      4      7      5.56      0.10  WORD_FREQ_CREDIT
     13     71    396      1      6      2.38      0.18  WORD_FREQ_CS
      2     40    268      6      7      6.50      0.01  WORD_FREQ_TELNET
      1      7      7      7      7      7.00      0.00  WORD_FREQ_PARTS
      0      0      0      0      0      0.00      0.00  WORD_FREQ_TABLE
      0      0      0      0      0      0.00      0.00  WORD_FREQ_415
      0      0      0      0      0      0.00      0.00  WORD_FREQ_857
      0      0      0      0      0      0.00      0.00  WORD_FREQ_ADDRESSES

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     9 terminal nodes
    Largest :    51 terminal nodes
    Average :     24.49000 terminal nodes

 Reconciling 2301 Learn sample scores across 5 selected models,
 the largest having 198 trees, to compute gains and PS tables.

 Reconciling 1150 Test sample scores across 5 selected models,
 the largest having 198 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 147

 Target: SPAM_OR_NOTSPAM
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     2301      2301.00
 N Test  Obs:     1150      1150.00
 Learn Rate :    0.1000000

 Storage requirements: 19192 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL          147      0.12786
                  ROC          193      0.98901
                 Lift           29      2.53863
              KS-stat          189      0.91877
          Class.Error          198      0.04000

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.59776     0.60291     0.96009     0.93976     2.53515     2.42175     0.83968     0.79692     0.39418     0.39391
      3     0.49032     0.50522     0.97157     0.95216     2.53693     2.47389     0.85105     0.81480     0.07953     0.12174
     10     0.29092     0.32688     0.98420     0.97142     2.53693     2.51656     0.89303     0.83753     0.05476     0.08957
     20     0.17229     0.22340     0.99219     0.98033     2.53693     2.51656     0.91728     0.86833     0.03651     0.06522
     29     0.12177     0.18158     0.99523     0.98376     2.53693     2.53863     0.94551     0.87914     0.03216     0.05913
     30     0.11782     0.17854     0.99541     0.98391     2.53693     2.53863     0.94617     0.87992     0.03129     0.06174
     40     0.08743     0.15595     0.99721     0.98594     2.53693     2.53863     0.95886     0.89460     0.02086     0.05130
     50     0.07228     0.14639     0.99805     0.98684     2.53693     2.51656     0.96691     0.89912     0.01738     0.05130
     60     0.06279     0.14098     0.99846     0.98739     2.53693     2.51656     0.97088     0.90343     0.01434     0.04957
     70     0.05452     0.13765     0.99902     0.98760     2.53693     2.53863     0.97634     0.90453     0.01173     0.04696
     80     0.05008     0.13481     0.99917     0.98799     2.53693     2.53863     0.97965     0.90343     0.01130     0.04696
     90     0.04339     0.13308     0.99952     0.98815     2.53693     2.53863     0.98467     0.90674     0.01000     0.04609
    100     0.03723     0.13230     0.99972     0.98812     2.53693     2.51656     0.98726     0.90607     0.00652     0.04522
    110     0.03306     0.13074     0.99981     0.98833     2.53693     2.51656     0.99013     0.90685     0.00522     0.04522
    120     0.02922     0.13067     0.99989     0.98831     2.53693     2.51656     0.99382     0.91369     0.00391     0.04435
    130     0.02515     0.12944     0.99994     0.98847     2.53693     2.51656     0.99636     0.91347     0.00304     0.04348
    140     0.02202     0.12806     0.99996     0.98857     2.53693     2.51656     0.99675     0.91479     0.00261     0.04087
    147     0.02025     0.12786     0.99997     0.98856     2.53693     2.51656     0.99746     0.91435     0.00174     0.04174
    150     0.01942     0.12861     0.99997     0.98856     2.53693     2.51656     0.99818     0.91502     0.00174     0.04261
    160     0.01733     0.12841     0.99998     0.98865     2.53693     2.51656     0.99818     0.91502     0.00130     0.04174
    170     0.01575     0.12851     0.99998     0.98881     2.53693     2.51656     0.99818     0.91623     0.00130     0.04174
    180     0.01388     0.12902     0.99999     0.98879     2.53693     2.51656     0.99818     0.91590     0.00087     0.04174
    189     0.01280     0.12917     1.00000     0.98896     2.53693     2.51656     0.99818     0.91877     0.00087     0.04348
    190     0.01269     0.12918     1.00000     0.98893     2.53693     2.51656     0.99818     0.91877     0.00087     0.04348
    193     0.01219     0.12918     1.00000     0.98901     2.53693     2.51656     0.99818     0.91711     0.00087     0.04348
    198     0.01156     0.13000     1.00000     0.98896     2.53693     2.51656     0.99857     0.91733     0.00087     0.04000
    200     0.01132     0.13060     1.00000     0.98891     2.53693     2.51656     0.99928     0.91711     0.00087     0.04087
    210     0.01002     0.13267     1.00000     0.98867     2.53693     2.51656     0.99928     0.91490     0.00087     0.04348
    220     0.00910     0.13395     1.00000     0.98860     2.53693     2.51656     0.99928     0.91568     0.00087     0.04087
    230     0.00815     0.13465     1.00000     0.98870     2.53693     2.51656     0.99928     0.91722     0.00087     0.04087
    240     0.00744     0.13635     1.00000     0.98863     2.53693     2.51656     0.99928     0.91645     0.00087     0.04174
    250     0.00670     0.13761     1.00000     0.98866     2.53693     2.51656     0.99928     0.91502     0.00087     0.04174
    260     0.00600     0.13883     1.00000     0.98862     2.53693     2.51656     0.99928     0.91270     0.00087     0.04174
    267     0.00562     0.14008     1.00000     0.98855     2.53693     2.51656     0.99928     0.91557     0.00043     0.04174
    270     0.00550     0.14058     1.00000     0.98850     2.53693     2.51656     0.99928     0.91347     0.00043     0.04174
    272     0.00540     0.14099     1.00000     0.98845     2.53693     2.51656     0.99928     0.91171     0.00043     0.04174
    280     0.00504     0.14210     1.00000     0.98841     2.53693     2.51656     0.99928     0.91170     0.00043     0.04261
    290     0.00461     0.14373     1.00000     0.98835     2.53693     2.51656     0.99928     0.91248     0.00043     0.04348
    300     0.00422     0.14539     1.00000     0.98827     2.53693     2.51656     0.99928     0.91270     0.00043     0.04348
    310     0.00392     0.14743     1.00000     0.98812     2.53693     2.51656     0.99928     0.91391     0.00043     0.04435
    320     0.00356     0.14924     1.00000     0.98816     2.53693     2.51656     0.99928     0.91458     0.00043     0.04435
    330     0.00324     0.15034     1.00000     0.98820     2.53693     2.51656     0.99928     0.91248     0.00043     0.04435
    340     0.00297     0.15203     1.00000     0.98822     2.53693     2.51656     0.99928     0.91314     0.00043     0.04435
    350     0.00277     0.15343     1.00000     0.98820     2.53693     2.51656     0.99928     0.91192     0.00043     0.04435
    360     0.00258     0.15465     1.00000     0.98815     2.53693     2.51656     0.99928     0.91215     0.00043     0.04348
    370     0.00241     0.15644     1.00000     0.98805     2.53693     2.51656     0.99928     0.91369     0.00043     0.04348
    380     0.00225     0.15779     1.00000     0.98804     2.53693     2.51656     0.99928     0.91490     0.00043     0.04261
    390     0.00208     0.15876     1.00000     0.98814     2.53693     2.51656     0.99928     0.91490     0.00043     0.04261
    400     0.00192     0.16047     1.00000     0.98810     2.53693     2.51656     0.99928     0.91634     0.00043     0.04261


 ==========================================
 Variable Importance for the 147-tree Model
 ==========================================

                                     Abs     Rel

 CHAR_FREQ_DOLLAR              100.00000  100.00 |***********|
 WORD_FREQ_REMOVE               66.44771   66.45 |********   |
 CHAR_FREQ_EXCLAMATION          60.39140   60.39 |*******    |
 WORD_FREQ_HP                   43.74175   43.74 |*****      |
 WORD_FREQ_FREE                 37.98513   37.99 |*****      |
 CAPITAL_RUN_LENGTH_TOTAL       34.11886   34.12 |****       |
 CAPITAL_RUN_LENGTH_AVERAGE     34.04745   34.05 |****       |
 CAPITAL_RUN_LENGTH_LONGEST     32.55905   32.56 |****       |
 WORD_FREQ_OUR                  23.97469   23.97 |***        |
 WORD_FREQ_YOUR                 23.85966   23.86 |***        |
 WORD_FREQ_EDU                  23.70346   23.70 |***        |
 WORD_FREQ_000                  20.72893   20.73 |***        |
 WORD_FREQ_GEORGE               20.40300   20.40 |***        |
 WORD_FREQ_YOU                  20.38822   20.39 |***        |
 WORD_FREQ_1999                 17.11018   17.11 |***        |
 CHAR_FREQ_BRACKET              16.92434   16.92 |***        |
 WORD_FREQ_WILL                 15.39961   15.40 |**         |
 WORD_FREQ_BUSINESS             13.85339   13.85 |**         |
 WORD_FREQ_INTERNET             13.71177   13.71 |**         |
 WORD_FREQ_RE                   12.88681   12.89 |**         |
 WORD_FREQ_MAIL                 11.68848   11.69 |**         |
 WORD_FREQ_650                  11.04451   11.04 |**         |
 WORD_FREQ_MEETING              10.81600   10.82 |**         |
 WORD_FREQ_RECEIVE              10.71795   10.72 |**         |
 WORD_FREQ_EMAIL                 8.98738    8.99 |**         |
 WORD_FREQ_ALL                   8.35729    8.36 |**         |
 WORD_FREQ_MONEY                 7.05682    7.06 |**         |
 WORD_FREQ_DATA                  6.65648    6.66 |**         |
 CHAR_FREQ_SEMICOLON             6.46789    6.47 |**         |
 WORD_FREQ_HPL                   6.11760    6.12 |**         |
 WORD_FREQ_TECHNOLOGY            5.71572    5.72 |**         |
 WORD_FREQ_DIRECT                4.93682    4.94 |*          |
 WORD_FREQ_FONT                  4.55583    4.56 |*          |
 WORD_FREQ_OVER                  4.44722    4.45 |*          |
 WORD_FREQ_PROJECT               4.22762    4.23 |*          |
 WORD_FREQ_MAKE                  4.19161    4.19 |*          |
 WORD_FREQ_85                    4.16471    4.16 |*          |
 WORD_FREQ_PM                    4.01055    4.01 |*          |
 WORD_FREQ_ADDRESS               3.52601    3.53 |*          |
 WORD_FREQ_PEOPLE                3.14941    3.15 |*          |
 WORD_FREQ_REPORT                2.73773    2.74 |*          |
 CHAR_FREQ_HASH                  2.68008    2.68 |*          |
 WORD_FREQ_ORDER                 2.46864    2.47 |*          |
 WORD_FREQ_LABS                  2.32893    2.33 |*          |
 WORD_FREQ_CONFERENCE            2.14727    2.15 |*          |
 WORD_FREQ_3D                    2.07309    2.07 |*          |
 CHAR_FREQ_BOXBRACKET            2.03889    2.04 |*          |
 WORD_FREQ_CREDIT                1.51248    1.51 |*          |
 WORD_FREQ_LAB                   1.01604    1.02 |*          |
 WORD_FREQ_PARTS                 0.92050    0.92 |*          |
 WORD_FREQ_ORIGINAL              0.68568    0.69 |*          |
 WORD_FREQ_CS                    0.64232    0.64 |*          |
 WORD_FREQ_TELNET                0.02813    0.03 |*          |


 Learn Sample Misclassification by Target Class
 For The 147-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 1394.00      1393.00         1.00       0.0007
 1                  907.00       904.00         3.00       0.0033


 Test Sample Misclassification by Target Class
 For The 147-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                  697.00       673.00        24.00       0.0344
 1                  453.00       429.00        24.00       0.0530

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                1.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               1.000 sec ( 0.00 hrs, 100.00%)
    Core model:         1.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             0.000 sec ( 0.00 hrs,   0.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000000 hrs  0.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 905 kb , 75% compression

 Grove file created containing:
      1 TreeNet

