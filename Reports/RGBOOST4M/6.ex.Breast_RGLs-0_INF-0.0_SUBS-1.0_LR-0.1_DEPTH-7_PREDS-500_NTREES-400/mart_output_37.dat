
 The "USE "../Data/Classification/6.ex.Breast/SAMPLES4/d" command: 00:00:00

 Model (target and predictors) reset: FATE

 The KEEP list has 4 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 1652
 Records Kept in Learning sample: 1652

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 826
 Records Kept in Test sample: 826

 Discrete         N Levels
 Variable         in Model
 -------------------------
 FATE                    2

 ======================
 Target Frequency Table
 ======================

 Variable: FATE
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         1584  95.88              1584.00  95.88
                 T         (792  95.88)             (792.00  95.88)
 1               L           68   4.12                68.00   4.12
                 T          (34   4.12)              (34.00   4.12)
 -----------------------------------------------------------------
 Totals
 0                         2376  95.88              2376.00  95.88
 1                          102   4.12               102.00   4.12
 -----------------------------------------------------------------
 Total                     2478                     2478.00
 Total Learn               1652                     1652.00
 Total Test                 826                      826.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.14940      0.16318      0.04116      0.04116      1.00000 |                                               *
      2       0.13960      0.15803      0.04116      0.04116      1.00000 |                                             *
      3       0.13216      0.15379      0.04116      0.04116      1.00000 |                                            *
      4       0.12630      0.15102      0.04116      0.04116      1.00000 |                                           *
      5       0.12142      0.14982      0.04116      0.04116      1.00000 |                                           *
      6       0.11727      0.14894      0.04056      0.03995      1.00000 |                                           *
      7       0.11387      0.14763      0.03935      0.03995      1.00000 |                                          *
      8       0.11076      0.14641      0.03814      0.03995      1.00000 |                                          *
      9       0.10609      0.14684      0.03814      0.03995      1.00000 |                                          *
     10       0.10407      0.14694      0.03753      0.03995      1.00000 |                                          *
     11       0.10204      0.14664      0.03692      0.04116      1.00000 |                                          *
     12       0.10058      0.14674      0.03692      0.04237      1.00000 |                                          *
     13       0.09911      0.14680      0.03571      0.04237      1.00000 |                                          *
     14       0.09768      0.14682      0.03511      0.04358      1.00000 |                                          *
     15       0.09644      0.14771      0.03390      0.04237      1.00000 |                                          *
     16       0.09542      0.14750      0.03329      0.04237      1.00000 |                                          *
     17       0.09445      0.14804      0.03208      0.04237      1.00000 |                                           *
     18       0.09293      0.14896      0.03087      0.04237      1.00000 |                                           *
     19       0.09143      0.14927      0.03087      0.04237      1.00000 |                                           *
     20       0.09012      0.15067      0.02966      0.04237      1.00000 |                                           *
     30       0.08419      0.15373      0.02482      0.04600      1.00000 |                                            *
     40       0.07632      0.15557      0.02240      0.04722      1.00000 |                                             *
     50       0.06933      0.15440      0.02179      0.04843      1.00000 |                                            *
     60       0.06567      0.15503      0.02240      0.04600      1.00000 |                                             *
     70       0.06272      0.15609      0.02179      0.04358      1.00000 |                                             *
     80       0.06176      0.15670      0.01937      0.04600      1.00000 |                                             *
     90       0.05986      0.15779      0.01937      0.04479      1.00000 |                                             *
    100       0.05591      0.15959      0.01755      0.04479      1.00000 |                                              *
    110       0.05300      0.16046      0.01695      0.04479      1.00000 |                                              *
    120       0.04831      0.16128      0.01574      0.04479      1.00000 |                                              *
    130       0.04524      0.16272      0.01513      0.04600      1.00000 |                                               *
    140       0.04287      0.16598      0.01453      0.04843      1.00000 |                                               *
    150       0.04005      0.16877      0.01453      0.04722      1.00000 |                                               *
    160       0.03829      0.16941      0.01392      0.04722      1.00000 |                                               *
    170       0.03715      0.17102      0.01392      0.04722      1.00000 |                                               *
    180       0.03485      0.17251      0.01271      0.04722      1.00000 |                                               *
    190       0.03340      0.17428      0.01271      0.04722      1.00000 |                                               *
    200       0.03224      0.17562      0.01150      0.04722      1.00000 |                                               *
    210       0.03062      0.17677      0.00969      0.04722      1.00000 |                                               *
    220       0.02875      0.17846      0.00847      0.04722      1.00000 |                                               *
    230       0.02750      0.17963      0.00847      0.04722      1.00000 |                                               *
    240       0.02701      0.18031      0.00787      0.04722      1.00000 |                                               *
    250       0.02506      0.18356      0.00726      0.04722      1.00000 |                                               *
    260       0.02370      0.18431      0.00545      0.04722      1.00000 |                                               *
    270       0.02202      0.18679      0.00363      0.04600      1.00000 |                                               *
    280       0.02104      0.18810      0.00363      0.04722      1.00000 |                                               *
    290       0.02017      0.18970      0.00363      0.04722      1.00000 |                                               *
    300       0.01934      0.19155      0.00303      0.04722      1.00000 |                                               *
    310       0.01853      0.19354      0.00182      0.04722      1.00000 |                                               *
    320       0.01798      0.19469      0.00182      0.04600      1.00000 |                                               *
    330       0.01695      0.19630      0.00182      0.04722      1.00000 |                                               *
    340       0.01608      0.19893      0.00121      0.04722      1.00000 |                                               *
    350       0.01542      0.20033      0.00121      0.04722      1.00000 |                                               *
    360       0.01476      0.20197      0.00121      0.04722      1.00000 |                                               *
    370       0.01420      0.20212      0.00121      0.04722      1.00000 |                                               *
    380       0.01337      0.20407      0.00121      0.04843      1.00000 |                                               *
    390       0.01304      0.20472      0.00121      0.04843      1.00000 |                                               *
    400       0.01258      0.20507      0.00121      0.04722      1.00000 |                                               *

 Core TN model building:          0.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    400      1    400      1      3      1.26      6.75  FOLLOW
    336      1    400      1      7      3.45      3.83  ENTAGE
    162      1    399      1      7      4.70      1.34  PD
     47      1    399      1      7      4.19      0.45  FH

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     8 terminal nodes
    Largest :    47 terminal nodes
    Average :     15.41500 terminal nodes

 Reconciling 1652 Learn sample scores across 5 selected models,
 the largest having 8 trees, to compute gains and PS tables.

 Reconciling 826 Test sample scores across 5 selected models,
 the largest having 8 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 8

 Target: FATE
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     1652      1652.00
 N Test  Obs:      826       826.00
 Learn Rate :    0.1000000

 Storage requirements: 11932 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL            8      0.14641
                  ROC            4      0.81200
                 Lift            5      4.41176
              KS-stat            8      0.58200
          Class.Error            6      0.03995

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.14940     0.16318     0.92706     0.59663     6.86765     3.52941     0.71713     0.27718     0.04116     0.04116
      4     0.12630     0.15102     0.93059     0.81200     7.20588     4.11765     0.75509     0.58073     0.04116     0.04116
      5     0.12142     0.14982     0.93950     0.80550     7.64706     4.41176     0.76582     0.55637     0.04116     0.04116
      6     0.11727     0.14894     0.94242     0.80717     7.32941     4.11765     0.75824     0.55043     0.04056     0.03995
      8     0.11076     0.14641     0.94827     0.81088     8.08824     4.41176     0.76185     0.58200     0.03814     0.03995
     10     0.10407     0.14694     0.95545     0.79869     8.23529     4.11765     0.76753     0.54880     0.03753     0.03995
     20     0.09012     0.15067     0.95867     0.77347     8.38235     3.82353     0.77889     0.49807     0.02966     0.04237
     30     0.08419     0.15373     0.96082     0.76367     8.38235     3.82353     0.78710     0.50349     0.02482     0.04600
     40     0.07632     0.15557     0.97497     0.76268     8.67647     3.82353     0.82104     0.48960     0.02240     0.04722
     50     0.06933     0.15440     0.98280     0.78197     9.11765     4.11765     0.86534     0.51255     0.02179     0.04843
     60     0.06567     0.15503     0.98613     0.77904     9.26471     4.11765     0.87968     0.47824     0.02240     0.04600
     70     0.06272     0.15609     0.98783     0.77583     9.26471     4.11765     0.88599     0.47898     0.02179     0.04358
     80     0.06176     0.15670     0.98820     0.77438     9.26471     3.82353     0.88789     0.48024     0.01937     0.04600
     90     0.05986     0.15779     0.98934     0.76942     9.55882     3.82353     0.90114     0.48945     0.01937     0.04479
    100     0.05591     0.15959     0.99219     0.76762     9.70588     3.82353     0.91945     0.50245     0.01755     0.04479
    110     0.05300     0.16046     0.99370     0.76550     9.70588     3.82353     0.92053     0.49740     0.01695     0.04479
    115     0.04922     0.16167     0.99670     0.76714    10.00000     3.82353     0.94975     0.47178     0.01574     0.04600
    120     0.04831     0.16128     0.99693     0.77353     9.85294     3.82353     0.95354     0.51634     0.01574     0.04479
    130     0.04524     0.16272     0.99812     0.77616    10.00000     3.82353     0.96635     0.46962     0.01513     0.04600
    140     0.04287     0.16598     0.99847     0.77171    10.00000     4.11765     0.97204     0.45663     0.01453     0.04843
    150     0.04005     0.16877     0.99896     0.76695    10.00000     4.11765     0.97961     0.45700     0.01453     0.04722
    160     0.03829     0.16941     0.99942     0.76862    10.00000     4.05882     0.99495     0.44400     0.01392     0.04722
    170     0.03715     0.17102     0.99945     0.76086    10.00000     4.11765     0.99558     0.44452     0.01392     0.04722
    180     0.03485     0.17251     0.99960     0.76142    10.00000     4.11765     0.99747     0.43821     0.01271     0.04722
    190     0.03340     0.17428     0.99970     0.75381    10.00000     3.82353     0.99747     0.44125     0.01271     0.04722
    200     0.03224     0.17562     0.99975     0.75032    10.00000     3.82353     0.99747     0.43620     0.01150     0.04722
    210     0.03062     0.17677     0.99980     0.75447    10.00000     3.82353     0.99747     0.44125     0.00969     0.04722
    220     0.02875     0.17846     0.99981     0.75499    10.00000     3.70588     0.99747     0.44615     0.00847     0.04722
    230     0.02750     0.17963     0.99985     0.75223    10.00000     3.82353     0.99747     0.44236     0.00847     0.04722
    240     0.02701     0.18031     0.99985     0.75052    10.00000     3.70588     0.99747     0.44363     0.00787     0.04722
    250     0.02506     0.18356     0.99987     0.74814    10.00000     4.11765     0.99811     0.43999     0.00726     0.04722
    255     0.02402     0.18447     0.99993     0.74937    10.00000     3.52941     0.99874     0.44125     0.00605     0.04722
    260     0.02370     0.18431     0.99993     0.75186    10.00000     3.70588     0.99874     0.44125     0.00545     0.04722
    270     0.02202     0.18679     0.99995     0.74517    10.00000     4.00000     0.99874     0.43873     0.00363     0.04600
    279     0.02105     0.18812     0.99998     0.74695    10.00000     3.52941     0.99874     0.43873     0.00363     0.04722
    280     0.02104     0.18810     0.99998     0.74625    10.00000     3.82353     0.99874     0.43873     0.00363     0.04722
    290     0.02017     0.18970     0.99998     0.74432    10.00000     3.82353     0.99874     0.43746     0.00363     0.04722
    300     0.01934     0.19155     0.99998     0.74164    10.00000     3.82353     0.99874     0.44504     0.00303     0.04722
    310     0.01853     0.19354     0.99998     0.74112    10.00000     3.82353     0.99874     0.43620     0.00182     0.04722
    320     0.01798     0.19469     0.99998     0.74261    10.00000     3.82353     0.99874     0.43494     0.00182     0.04600
    330     0.01695     0.19630     0.99998     0.74283    10.00000     3.82353     0.99874     0.43367     0.00182     0.04722
    338     0.01649     0.19794     0.99998     0.73849    10.00000     3.82353     0.99874     0.41563     0.00121     0.04722
    340     0.01608     0.19893     0.99998     0.74060    10.00000     3.52941     0.99874     0.42320     0.00121     0.04722
    350     0.01542     0.20033     0.99998     0.74294    10.00000     3.52941     0.99874     0.42610     0.00121     0.04722
    360     0.01476     0.20197     0.99998     0.74287    10.00000     3.23529     0.99874     0.43115     0.00121     0.04722
    370     0.01420     0.20212     0.99998     0.74532    10.00000     3.52941     0.99874     0.43115     0.00121     0.04722
    380     0.01337     0.20407     0.99998     0.74376    10.00000     3.82353     0.99874     0.42736     0.00121     0.04843
    390     0.01304     0.20472     0.99998     0.74432    10.00000     3.82353     0.99874     0.42610     0.00121     0.04843
    400     0.01258     0.20507     0.99998     0.74521    10.00000     3.82353     0.99874     0.41763     0.00121     0.04722


 ========================================
 Variable Importance for the 8-tree Model
 ========================================

                 Abs     Rel

 FOLLOW    100.00000  100.00 |***********|
 ENTAGE     61.10108   61.10 |*******    |
 PD         30.94134   30.94 |****       |
 FH         14.15336   14.15 |**         |


 Learn Sample Misclassification by Target Class
 For The 8-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 1584.00      1584.00         0.00       0.0000
 1                   68.00         5.00        63.00       0.9265


 Test Sample Misclassification by Target Class
 For The 8-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                  792.00       792.00         0.00       0.0000
 1                   34.00         1.00        33.00       0.9706

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                0.000 sec ( 0.00 hrs)

 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 521 kb , 78% compression

 Grove file created containing:
      1 TreeNet

