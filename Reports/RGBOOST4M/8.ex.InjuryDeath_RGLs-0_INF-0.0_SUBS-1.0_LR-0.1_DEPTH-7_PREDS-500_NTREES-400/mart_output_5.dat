
 The "USE "../Data/Classification/8.ex.InjuryDeath/SAMPL" command: 00:00:00

 Model (target and predictors) reset: ANY_INJ_DTH

 The KEEP list has 10 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 7594
 Records Kept in Learning sample: 7594

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 3796
 Records Kept in Test sample: 3796

 Discrete         N Levels
 Variable         in Model
 -------------------------
 ANY_INJ_DTH             2

 ======================
 Target Frequency Table
 ======================

 Variable: ANY_INJ_DTH
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         7373  97.09              7373.00  97.09
                 T        (3686  97.10)            (3686.00  97.10)
 1               L          221   2.91               221.00   2.91
                 T         (110   2.90)             (110.00   2.90)
 -----------------------------------------------------------------
 Totals
 0                        11059  97.09             11059.00  97.09
 1                          331   2.91               331.00   2.91
 -----------------------------------------------------------------
 Total                    11390                    11390.00
 Total Learn               7594                     7594.00
 Total Test                3796                     3796.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.11055      0.11916      0.02910      0.02898      1.00000 |                                               *
      2       0.10319      0.11524      0.02910      0.02898      1.00000 |                                             *
      3       0.09835      0.11271      0.02910      0.02898      1.00000 |                                            *
      4       0.09383      0.11070      0.02910      0.02898      1.00000 |                                            *
      5       0.09037      0.10925      0.02910      0.02898      1.00000 |                                           *
      6       0.08755      0.10791      0.02910      0.02898      1.00000 |                                          *
      7       0.08501      0.10670      0.02858      0.02819      1.00000 |                                          *
      8       0.08284      0.10619      0.02805      0.02766      1.00000 |                                          *
      9       0.08055      0.10542      0.02713      0.02713      1.00000 |                                         *
     10       0.07904      0.10471      0.02607      0.02713      1.00000 |                                         *
     11       0.07723      0.10438      0.02555      0.02687      1.00000 |                                         *
     12       0.07553      0.10418      0.02515      0.02740      1.00000 |                                         *
     13       0.07431      0.10381      0.02515      0.02766      1.00000 |                                         *
     14       0.07319      0.10350      0.02489      0.02766      1.00000 |                                         *
     15       0.07183      0.10322      0.02541      0.02766      1.00000 |                                         *
     16       0.07039      0.10289      0.02462      0.02766      1.00000 |                                        *
     17       0.06975      0.10268      0.02476      0.02766      1.00000 |                                        *
     18       0.06864      0.10262      0.02449      0.02792      1.00000 |                                        *
     19       0.06812      0.10253      0.02423      0.02819      1.00000 |                                        *
     20       0.06765      0.10248      0.02397      0.02819      1.00000 |                                        *
     30       0.06360      0.10263      0.02225      0.02871      1.00000 |                                        *
     40       0.06226      0.10317      0.02173      0.03030      1.00000 |                                         *
     50       0.06054      0.10353      0.02133      0.03056      1.00000 |                                         *
     60       0.05837      0.10414      0.02028      0.03030      1.00000 |                                         *
     70       0.05553      0.10462      0.01988      0.03082      1.00000 |                                         *
     80       0.05443      0.10512      0.01923      0.03082      1.00000 |                                         *
     90       0.05342      0.10553      0.01896      0.03161      1.00000 |                                          *
    100       0.05276      0.10602      0.01870      0.03214      1.00000 |                                          *
    110       0.05134      0.10652      0.01791      0.03161      1.00000 |                                          *
    120       0.05007      0.10705      0.01738      0.03161      1.00000 |                                          *
    130       0.04768      0.10749      0.01712      0.03188      1.00000 |                                          *
    140       0.04695      0.10772      0.01686      0.03188      1.00000 |                                          *
    150       0.04510      0.10820      0.01593      0.03188      1.00000 |                                           *
    160       0.04254      0.10879      0.01528      0.03161      1.00000 |                                           *
    170       0.04219      0.10929      0.01541      0.03161      1.00000 |                                           *
    180       0.04160      0.10948      0.01475      0.03135      1.00000 |                                           *
    190       0.03879      0.11019      0.01383      0.03240      1.00000 |                                           *
    200       0.03698      0.11077      0.01304      0.03240      1.00000 |                                            *
    210       0.03524      0.11139      0.01198      0.03267      1.00000 |                                            *
    220       0.03442      0.11185      0.01106      0.03293      1.00000 |                                            *
    230       0.03365      0.11264      0.01053      0.03319      1.00000 |                                            *
    240       0.03223      0.11337      0.01053      0.03293      1.00000 |                                             *
    250       0.03140      0.11402      0.01014      0.03319      1.00000 |                                             *
    260       0.03085      0.11456      0.01014      0.03319      1.00000 |                                             *
    270       0.02935      0.11542      0.00961      0.03267      1.00000 |                                             *
    280       0.02724      0.11679      0.00856      0.03319      1.00000 |                                              *
    290       0.02637      0.11775      0.00830      0.03267      1.00000 |                                              *
    300       0.02518      0.11883      0.00790      0.03293      1.00000 |                                               *
    310       0.02364      0.11968      0.00724      0.03267      1.00000 |                                               *
    320       0.02200      0.12071      0.00606      0.03293      1.00000 |                                               *
    330       0.02072      0.12181      0.00500      0.03293      1.00000 |                                               *
    340       0.02005      0.12290      0.00421      0.03293      1.00000 |                                               *
    350       0.01966      0.12365      0.00395      0.03293      1.00000 |                                               *
    360       0.01891      0.12465      0.00395      0.03293      1.00000 |                                               *
    370       0.01843      0.12545      0.00395      0.03319      1.00000 |                                               *
    380       0.01804      0.12595      0.00356      0.03346      1.00000 |                                               *
    390       0.01770      0.12686      0.00342      0.03346      1.00000 |                                               *
    400       0.01733      0.12720      0.00316      0.03398      1.00000 |                                               *

 Core TN model building:          1.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    400      1    400      1      3      1.11      6.90  CHILDYRS
    278      1    400      1      7      4.42      2.49  AGE
    248      1    400      1      7      4.93      1.91  INCOME
    239      1    400      2      7      4.69      1.98  AGE_MOM
    197      1    400      2      7      5.03      1.46  EDUC_MOM
    182      1    400      2      7      5.37      1.20  OTH_CHLD
    129      1    392      1      7      5.26      0.88  ILLEGIT
     80      1    396      2      7      5.64      0.47  RACE_MOM
     79      3    396      2      7      5.84      0.43  PNCLATE
     70      1    400      3      7      5.80      0.39  LBW

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     8 terminal nodes
    Largest :    66 terminal nodes
    Average :     21.68000 terminal nodes

 Reconciling 7594 Learn sample scores across 5 selected models,
 the largest having 38 trees, to compute gains and PS tables.

 Reconciling 3796 Test sample scores across 5 selected models,
 the largest having 38 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 22

 Target: ANY_INJ_DTH
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     7594      7594.00
 N Test  Obs:     3796      3796.00
 Learn Rate :    0.1000000

 Storage requirements: 16944 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL           22      0.10230
                  ROC           14      0.86058
                 Lift           38      5.90909
              KS-stat          110      0.57808
          Class.Error           11      0.02687

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.11055     0.11916     0.92684     0.76575     7.25212     4.94443     0.67944     0.45568     0.02910     0.02898
     10     0.07904     0.10471     0.95507     0.85318     8.59729     5.27273     0.78841     0.56585     0.02607     0.02713
     11     0.07723     0.10438     0.95688     0.85804     8.64253     5.27273     0.79936     0.56653     0.02555     0.02687
     14     0.07319     0.10350     0.95947     0.86058     8.73303     5.36364     0.81184     0.55961     0.02489     0.02766
     20     0.06765     0.10248     0.96459     0.85861     8.82353     5.54545     0.81514     0.55636     0.02397     0.02819
     22     0.06645     0.10230     0.96488     0.85947     8.73303     5.78182     0.81632     0.56043     0.02357     0.02819
     30     0.06360     0.10263     0.96721     0.85944     8.77828     5.81818     0.81483     0.55800     0.02225     0.02871
     38     0.06262     0.10320     0.96833     0.85762     8.82353     5.90909     0.82542     0.55703     0.02173     0.03030
     40     0.06226     0.10317     0.96845     0.85805     8.82353     5.81818     0.82836     0.55841     0.02173     0.03030
     50     0.06054     0.10353     0.97037     0.85907     8.95928     5.84545     0.84026     0.56247     0.02133     0.03056
     60     0.05837     0.10414     0.97463     0.85728     9.10407     5.81818     0.85215     0.56817     0.02028     0.03030
     70     0.05553     0.10462     0.97924     0.85599     9.50226     5.63636     0.88177     0.57469     0.01988     0.03082
     80     0.05443     0.10512     0.97988     0.85467     9.50226     5.54545     0.88150     0.57577     0.01923     0.03082
     90     0.05342     0.10553     0.98058     0.85520     9.54751     5.54545     0.88286     0.57455     0.01896     0.03161
    100     0.05276     0.10602     0.98103     0.85408     9.54751     5.63636     0.88258     0.57265     0.01870     0.03214
    110     0.05134     0.10652     0.98232     0.85432     9.50226     5.72727     0.88281     0.57808     0.01791     0.03161
    120     0.05007     0.10705     0.98351     0.85425     9.59276     5.72727     0.88902     0.56695     0.01738     0.03161
    130     0.04768     0.10749     0.98874     0.85605     9.63801     5.54545     0.90588     0.57361     0.01712     0.03188
    140     0.04695     0.10772     0.98916     0.85609     9.65611     5.63636     0.90665     0.56560     0.01686     0.03188
    150     0.04510     0.10820     0.99090     0.85694     9.77376     5.45455     0.92447     0.57292     0.01593     0.03188
    160     0.04254     0.10879     0.99233     0.85470     9.81900     5.72727     0.92886     0.56438     0.01528     0.03161
    170     0.04219     0.10929     0.99247     0.85377     9.81900     5.72727     0.92859     0.55923     0.01541     0.03161
    180     0.04160     0.10948     0.99267     0.85448     9.81900     5.54545     0.92913     0.55325     0.01475     0.03135
    190     0.03879     0.11019     0.99432     0.85475     9.86425     5.54545     0.94352     0.54768     0.01383     0.03240
    200     0.03698     0.11077     0.99565     0.85491     9.86425     5.54545     0.94745     0.56341     0.01304     0.03240
    210     0.03524     0.11139     0.99615     0.85282     9.86425     5.72727     0.95098     0.56449     0.01198     0.03267
    220     0.03442     0.11185     0.99639     0.85264     9.86425     5.54545     0.95369     0.56124     0.01106     0.03293
    230     0.03365     0.11264     0.99654     0.85075     9.86425     5.45455     0.95491     0.55568     0.01053     0.03319
    240     0.03223     0.11337     0.99732     0.84955     9.86425     5.54545     0.96006     0.55296     0.01053     0.03293
    250     0.03140     0.11402     0.99750     0.84850     9.86425     5.54545     0.96061     0.55242     0.01014     0.03319
    260     0.03085     0.11456     0.99757     0.84770     9.86425     5.54545     0.96156     0.55296     0.01014     0.03319
    264     0.02996     0.11510     0.99855     0.84668    10.00000     5.45455     0.96359     0.55105     0.00988     0.03319
    270     0.02935     0.11542     0.99862     0.84583    10.00000     5.54545     0.96373     0.55078     0.00961     0.03267
    280     0.02724     0.11679     0.99914     0.84427    10.00000     5.45455     0.97300     0.54849     0.00856     0.03319
    290     0.02637     0.11775     0.99922     0.84140    10.00000     5.45455     0.97259     0.54523     0.00830     0.03267
    300     0.02518     0.11883     0.99935     0.83975    10.00000     5.50909     0.97535     0.54591     0.00790     0.03293
    310     0.02364     0.11968     0.99966     0.83843    10.00000     5.36364     0.98059     0.54808     0.00724     0.03267
    320     0.02200     0.12071     0.99982     0.83722    10.00000     5.36364     0.98607     0.53981     0.00606     0.03293
    330     0.02072     0.12181     0.99991     0.83331    10.00000     5.45455     0.99181     0.54876     0.00500     0.03293
    340     0.02005     0.12290     0.99996     0.83231    10.00000     5.45455     0.99674     0.54618     0.00421     0.03293
    350     0.01966     0.12365     0.99996     0.83066    10.00000     5.27273     0.99702     0.54672     0.00395     0.03293
    360     0.01891     0.12465     0.99997     0.82925    10.00000     5.27273     0.99729     0.54170     0.00395     0.03293
    370     0.01843     0.12545     0.99997     0.82851    10.00000     5.32727     0.99810     0.53953     0.00395     0.03319
    380     0.01804     0.12595     0.99998     0.82674    10.00000     5.27273     0.99810     0.53885     0.00356     0.03346
    390     0.01770     0.12686     0.99998     0.82351    10.00000     5.27273     0.99824     0.53913     0.00342     0.03346
    396     0.01747     0.12707     0.99998     0.82418    10.00000     5.27273     0.99837     0.53764     0.00329     0.03398
    400     0.01733     0.12720     0.99998     0.82513    10.00000     5.18182     0.99837     0.54062     0.00316     0.03398


 =========================================
 Variable Importance for the 22-tree Model
 =========================================

                   Abs     Rel

 CHILDYRS    100.00000  100.00 |***********|
 AGE          41.78545   41.79 |*****      |
 ILLEGIT      35.13685   35.14 |****       |
 AGE_MOM      34.19198   34.19 |****       |
 INCOME       29.88620   29.89 |****       |
 EDUC_MOM     28.43709   28.44 |****       |
 OTH_CHLD     25.76034   25.76 |****       |
 RACE_MOM     17.21581   17.22 |***        |
 LBW           9.38604    9.39 |**         |
 PNCLATE       6.94664    6.95 |**         |


 Learn Sample Misclassification by Target Class
 For The 22-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 7373.00      7366.00         7.00       0.0009
 1                  221.00        49.00       172.00       0.7783


 Test Sample Misclassification by Target Class
 For The 22-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 3686.00      3675.00        11.00       0.0030
 1                  110.00        14.00        96.00       0.8727

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                2.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               1.000 sec ( 0.00 hrs,  50.00%)
    Core model:         1.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             1.000 sec ( 0.00 hrs,  50.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000278 hrs 50.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 796 kb , 76% compression

 Grove file created containing:
      1 TreeNet

