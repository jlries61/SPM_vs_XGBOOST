
 The "USE "../Data/Classification/8.ex.InjuryDeath/SAMPL" command: 00:00:01

 Model (target and predictors) reset: ANY_INJ_DTH

 The KEEP list has 10 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 7594
 Records Kept in Learning sample: 7594

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 3796
 Records Kept in Test sample: 3796

 Discrete         N Levels
 Variable         in Model
 -------------------------
 ANY_INJ_DTH             2

 ======================
 Target Frequency Table
 ======================

 Variable: ANY_INJ_DTH
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         7373  97.09              7373.00  97.09
                 T        (3686  97.10)            (3686.00  97.10)
 1               L          221   2.91               221.00   2.91
                 T         (110   2.90)             (110.00   2.90)
 -----------------------------------------------------------------
 Totals
 0                        11059  97.09             11059.00  97.09
 1                          331   2.91               331.00   2.91
 -----------------------------------------------------------------
 Total                    11390                    11390.00
 Total Learn               7594                     7594.00
 Total Test                3796                     3796.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.11104      0.11943      0.02910      0.02898      1.00000 |                                               *
      2       0.10312      0.11602      0.02910      0.02898      1.00000 |                                              *
      3       0.09828      0.11388      0.02910      0.02898      1.00000 |                                             *
      4       0.09411      0.11219      0.02910      0.02898      1.00000 |                                            *
      5       0.09073      0.11097      0.02910      0.02898      1.00000 |                                            *
      6       0.08738      0.10974      0.02844      0.02924      1.00000 |                                           *
      7       0.08445      0.10901      0.02726      0.03003      1.00000 |                                           *
      8       0.08179      0.10912      0.02660      0.03082      1.00000 |                                           *
      9       0.07970      0.10867      0.02581      0.03082      1.00000 |                                           *
     10       0.07789      0.10830      0.02515      0.03030      1.00000 |                                           *
     11       0.07630      0.10793      0.02502      0.03030      1.00000 |                                          *
     12       0.07459      0.10750      0.02476      0.03082      1.00000 |                                          *
     13       0.07316      0.10736      0.02462      0.03082      1.00000 |                                          *
     14       0.07186      0.10742      0.02397      0.03135      1.00000 |                                          *
     15       0.07089      0.10717      0.02383      0.03135      1.00000 |                                          *
     16       0.07005      0.10697      0.02370      0.03135      1.00000 |                                          *
     17       0.06878      0.10708      0.02344      0.03109      1.00000 |                                          *
     18       0.06811      0.10679      0.02344      0.03109      1.00000 |                                          *
     19       0.06715      0.10683      0.02331      0.03135      1.00000 |                                          *
     20       0.06669      0.10687      0.02265      0.03135      1.00000 |                                          *
     30       0.06167      0.10661      0.02094      0.03214      1.00000 |                                          *
     40       0.05794      0.10698      0.02015      0.03267      1.00000 |                                          *
     50       0.05626      0.10697      0.01949      0.03240      1.00000 |                                          *
     60       0.05493      0.10697      0.01909      0.03214      1.00000 |                                          *
     70       0.05328      0.10748      0.01870      0.03188      1.00000 |                                          *
     80       0.05161      0.10838      0.01830      0.03161      1.00000 |                                           *
     90       0.05006      0.10922      0.01844      0.03135      1.00000 |                                           *
    100       0.04928      0.10944      0.01817      0.03240      1.00000 |                                           *
    110       0.04867      0.11004      0.01725      0.03135      1.00000 |                                           *
    120       0.04831      0.10996      0.01725      0.03135      1.00000 |                                           *
    130       0.04728      0.11027      0.01699      0.03109      1.00000 |                                           *
    140       0.04629      0.11091      0.01659      0.03214      1.00000 |                                            *
    150       0.04575      0.11112      0.01672      0.03188      1.00000 |                                            *
    160       0.04520      0.11137      0.01646      0.03214      1.00000 |                                            *
    170       0.04412      0.11193      0.01593      0.03240      1.00000 |                                            *
    180       0.04320      0.11201      0.01554      0.03267      1.00000 |                                            *
    190       0.04214      0.11251      0.01514      0.03240      1.00000 |                                            *
    200       0.04129      0.11300      0.01449      0.03267      1.00000 |                                            *
    210       0.04054      0.11319      0.01396      0.03214      1.00000 |                                            *
    220       0.03954      0.11361      0.01343      0.03188      1.00000 |                                             *
    230       0.03875      0.11406      0.01356      0.03188      1.00000 |                                             *
    240       0.03790      0.11442      0.01317      0.03214      1.00000 |                                             *
    250       0.03654      0.11499      0.01277      0.03214      1.00000 |                                             *
    260       0.03570      0.11543      0.01225      0.03161      1.00000 |                                             *
    270       0.03488      0.11600      0.01198      0.03135      1.00000 |                                              *
    280       0.03328      0.11661      0.01119      0.03188      1.00000 |                                              *
    290       0.03258      0.11701      0.01093      0.03188      1.00000 |                                              *
    300       0.03157      0.11792      0.01067      0.03161      1.00000 |                                              *
    310       0.03063      0.11899      0.00988      0.03214      1.00000 |                                               *
    320       0.02998      0.11963      0.00948      0.03240      1.00000 |                                               *
    330       0.02916      0.12002      0.00935      0.03240      1.00000 |                                               *
    340       0.02843      0.12050      0.00935      0.03267      1.00000 |                                               *
    350       0.02790      0.12122      0.00895      0.03346      1.00000 |                                               *
    360       0.02721      0.12228      0.00830      0.03319      1.00000 |                                               *
    370       0.02654      0.12287      0.00816      0.03346      1.00000 |                                               *
    380       0.02633      0.12317      0.00790      0.03293      1.00000 |                                               *
    390       0.02561      0.12365      0.00764      0.03267      1.00000 |                                               *
    400       0.02510      0.12408      0.00724      0.03267      1.00000 |                                               *

 Core TN model building:          2.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    400      1    400      1      3      1.08      6.92  CHILDYRS
    224      1    398      1      7      4.99      1.69  AGE
    201      1    398      1      7      5.36      1.33  INCOME
    194      1    395      1      7      4.98      1.46  EDUC_MOM
    174      1    400      2      7      5.12      1.25  AGE_MOM
    173      1    400      2      7      5.23      1.20  OTH_CHLD
    147      1    397      1      7      5.21      1.03  ILLEGIT
     59      2    388      4      7      6.15      0.27  RACE_MOM
     50      1    395      4      7      6.00      0.25  PNCLATE
     49      1    394      4      7      5.94      0.25  LBW

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     8 terminal nodes
    Largest :    71 terminal nodes
    Average :     18.87750 terminal nodes

 Reconciling 7594 Learn sample scores across 4 selected models,
 the largest having 37 trees, to compute gains and PS tables.

 Reconciling 3796 Test sample scores across 4 selected models,
 the largest having 37 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 26

 Target: ANY_INJ_DTH
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     7594      7594.00
 N Test  Obs:     3796      3796.00
 Learn Rate :    0.1000000

 Storage requirements: 14702 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL           26      0.10629
                  ROC           37      0.85505
                 Lift           35      5.72727
              KS-stat           81      0.58607
          Class.Error            1      0.02898

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.11104     0.11943     0.93192     0.79414     7.50528     4.68951     0.70361     0.52242     0.02910     0.02898
     10     0.07789     0.10830     0.96058     0.84534     8.50679     5.23636     0.79212     0.55758     0.02515     0.03030
     20     0.06669     0.10687     0.96736     0.85183     8.86878     5.54545     0.81201     0.56642     0.02265     0.03135
     26     0.06317     0.10629     0.97199     0.85440     8.97738     5.45455     0.82841     0.56898     0.02160     0.03267
     30     0.06167     0.10661     0.97274     0.85416     9.06787     5.60000     0.83631     0.57711     0.02094     0.03214
     35     0.06013     0.10646     0.97440     0.85459     9.18552     5.72727     0.85359     0.58186     0.02041     0.03188
     37     0.05998     0.10640     0.97404     0.85505     9.18552     5.72727     0.85359     0.58186     0.02041     0.03188
     40     0.05794     0.10698     0.97823     0.85201     9.32127     5.72727     0.86335     0.58228     0.02015     0.03267
     50     0.05626     0.10697     0.98000     0.85216     9.32127     5.63636     0.86527     0.58201     0.01949     0.03240
     60     0.05493     0.10697     0.98076     0.85376     9.32127     5.63636     0.86964     0.57821     0.01909     0.03214
     70     0.05328     0.10748     0.98212     0.85106     9.45701     5.54545     0.87839     0.58038     0.01870     0.03188
     80     0.05161     0.10838     0.98418     0.84688     9.59276     5.54545     0.89075     0.57766     0.01830     0.03161
     81     0.05142     0.10875     0.98426     0.84662     9.59276     5.45455     0.88993     0.58607     0.01830     0.03135
     90     0.05006     0.10922     0.98706     0.84551     9.63801     5.36364     0.89839     0.57577     0.01844     0.03135
    100     0.04928     0.10944     0.98759     0.84580     9.63801     5.45455     0.89817     0.56777     0.01817     0.03240
    110     0.04867     0.11004     0.98783     0.84563     9.68326     5.09091     0.89803     0.56748     0.01725     0.03135
    120     0.04831     0.10996     0.98795     0.84683     9.68326     5.09091     0.89554     0.56384     0.01725     0.03135
    130     0.04728     0.11027     0.98935     0.84728     9.77376     5.00000     0.90909     0.56613     0.01699     0.03109
    140     0.04629     0.11091     0.99026     0.84633     9.77376     4.90909     0.91104     0.56667     0.01659     0.03214
    150     0.04575     0.11112     0.99058     0.84489     9.77376     4.81818     0.91204     0.56953     0.01672     0.03188
    160     0.04520     0.11137     0.99102     0.84533     9.77376     4.81818     0.91245     0.57143     0.01646     0.03214
    170     0.04412     0.11193     0.99163     0.84416     9.77376     4.90909     0.91372     0.57469     0.01593     0.03240
    180     0.04320     0.11201     0.99199     0.84386     9.77376     4.90909     0.91331     0.57794     0.01554     0.03267
    190     0.04214     0.11251     0.99274     0.84392     9.83710     4.90909     0.92271     0.57767     0.01514     0.03240
    200     0.04129     0.11300     0.99308     0.84266     9.86425     4.72727     0.92365     0.56465     0.01449     0.03267
    210     0.04054     0.11319     0.99338     0.84233     9.86425     4.72727     0.92460     0.55420     0.01396     0.03214
    220     0.03954     0.11361     0.99378     0.84215     9.86425     4.72727     0.92850     0.55352     0.01343     0.03188
    230     0.03875     0.11406     0.99432     0.84163     9.86425     4.72727     0.93728     0.55841     0.01356     0.03188
    240     0.03790     0.11442     0.99484     0.84143     9.90950     4.63636     0.93877     0.54877     0.01317     0.03214
    250     0.03654     0.11499     0.99622     0.84085     9.90950     4.63636     0.94519     0.55352     0.01277     0.03214
    260     0.03570     0.11543     0.99663     0.84044     9.90950     4.63636     0.95026     0.54945     0.01225     0.03161
    270     0.03488     0.11600     0.99691     0.83856     9.90950     4.45455     0.95093     0.54498     0.01198     0.03135
    280     0.03328     0.11661     0.99735     0.83765     9.90950     4.63636     0.95284     0.53968     0.01119     0.03188
    290     0.03258     0.11701     0.99753     0.83782     9.90950     4.69091     0.95433     0.53534     0.01093     0.03188
    300     0.03157     0.11792     0.99799     0.83623     9.90950     4.72727     0.95920     0.53534     0.01067     0.03161
    310     0.03063     0.11899     0.99826     0.83481     9.95475     4.63636     0.96468     0.53710     0.00988     0.03214
    320     0.02998     0.11963     0.99834     0.83323     9.95475     4.63636     0.96508     0.53466     0.00948     0.03240
    330     0.02916     0.12002     0.99871     0.83403     9.95475     4.63636     0.96952     0.52367     0.00935     0.03240
    334     0.02883     0.12006     0.99879     0.83506    10.00000     4.72727     0.97069     0.52708     0.00935     0.03240
    340     0.02843     0.12050     0.99885     0.83378    10.00000     4.72727     0.97083     0.51975     0.00935     0.03267
    350     0.02790     0.12122     0.99896     0.83137    10.00000     4.72727     0.97142     0.51730     0.00895     0.03346
    360     0.02721     0.12228     0.99905     0.82739    10.00000     4.72727     0.97405     0.50061     0.00830     0.03319
    370     0.02654     0.12287     0.99915     0.82560    10.00000     4.63636     0.97901     0.50685     0.00816     0.03346
    380     0.02633     0.12317     0.99917     0.82474    10.00000     4.63636     0.97956     0.50210     0.00790     0.03293
    390     0.02561     0.12365     0.99927     0.82216    10.00000     4.54545     0.98037     0.48881     0.00764     0.03267
    394     0.02545     0.12395     0.99928     0.82168    10.00000     4.54545     0.98064     0.49152     0.00751     0.03240
    395     0.02524     0.12407     0.99932     0.82202    10.00000     4.54545     0.97983     0.48460     0.00724     0.03267
    399     0.02511     0.12410     0.99933     0.82186    10.00000     4.54545     0.97996     0.48541     0.00724     0.03267
    400     0.02510     0.12408     0.99933     0.82192    10.00000     4.54545     0.97996     0.48541     0.00724     0.03267


 =========================================
 Variable Importance for the 26-tree Model
 =========================================

                   Abs     Rel

 CHILDYRS    100.00000  100.00 |***********|
 EDUC_MOM     43.15766   43.16 |*****      |
 AGE          41.40257   41.40 |*****      |
 AGE_MOM      34.41337   34.41 |****       |
 OTH_CHLD     30.19951   30.20 |****       |
 ILLEGIT      24.76029   24.76 |***        |
 INCOME       23.85342   23.85 |***        |
 PNCLATE      14.75591   14.76 |**         |
 LBW          12.68857   12.69 |**         |
 RACE_MOM      9.80997    9.81 |**         |


 Learn Sample Misclassification by Target Class
 For The 26-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 7373.00      7370.00         3.00       0.0004
 1                  221.00        60.00       161.00       0.7285


 Test Sample Misclassification by Target Class
 For The 26-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 3686.00      3666.00        20.00       0.0054
 1                  110.00         6.00       104.00       0.9455

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                2.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               2.000 sec ( 0.00 hrs, 100.00%)
    Core model:         2.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             0.000 sec ( 0.00 hrs,   0.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000000 hrs  0.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 676 kb , 76% compression

 Grove file created containing:
      1 TreeNet

