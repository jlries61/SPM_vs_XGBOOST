
 The "USE "../Data/Classification/8.ex.InjuryDeath/SAMPL" command: 00:00:00

 Model (target and predictors) reset: ANY_INJ_DTH

 The KEEP list has 10 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 7594
 Records Kept in Learning sample: 7594

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 3796
 Records Kept in Test sample: 3796

 Discrete         N Levels
 Variable         in Model
 -------------------------
 ANY_INJ_DTH             2

 ======================
 Target Frequency Table
 ======================

 Variable: ANY_INJ_DTH
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         7373  97.09              7373.00  97.09
                 T        (3686  97.10)            (3686.00  97.10)
 1               L          221   2.91               221.00   2.91
                 T         (110   2.90)             (110.00   2.90)
 -----------------------------------------------------------------
 Totals
 0                        11059  97.09             11059.00  97.09
 1                          331   2.91               331.00   2.91
 -----------------------------------------------------------------
 Total                    11390                    11390.00
 Total Learn               7594                     7594.00
 Total Test                3796                     3796.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.11210      0.11854      0.02910      0.02898      1.00000 |                                               *
      2       0.10464      0.11458      0.02910      0.02898      1.00000 |                                             *
      3       0.09971      0.11236      0.02910      0.02898      1.00000 |                                            *
      4       0.09528      0.11079      0.02910      0.02898      1.00000 |                                            *
      5       0.09200      0.10980      0.02910      0.02898      1.00000 |                                           *
      6       0.08926      0.10869      0.02910      0.02898      1.00000 |                                           *
      7       0.08671      0.10820      0.02792      0.02792      1.00000 |                                           *
      8       0.08453      0.10777      0.02792      0.02792      1.00000 |                                           *
      9       0.08235      0.10667      0.02792      0.02792      1.00000 |                                          *
     10       0.08014      0.10634      0.02792      0.02766      1.00000 |                                          *
     11       0.07832      0.10623      0.02792      0.02792      1.00000 |                                          *
     12       0.07662      0.10611      0.02713      0.02740      1.00000 |                                          *
     13       0.07509      0.10553      0.02647      0.02687      1.00000 |                                          *
     14       0.07386      0.10566      0.02607      0.02687      1.00000 |                                          *
     15       0.07271      0.10542      0.02594      0.02661      1.00000 |                                          *
     16       0.07161      0.10534      0.02555      0.02634      1.00000 |                                          *
     17       0.07036      0.10534      0.02515      0.02661      1.00000 |                                          *
     18       0.06966      0.10544      0.02515      0.02661      1.00000 |                                          *
     19       0.06901      0.10557      0.02515      0.02634      1.00000 |                                          *
     20       0.06806      0.10596      0.02502      0.02687      1.00000 |                                          *
     30       0.06235      0.10625      0.02291      0.02740      1.00000 |                                          *
     40       0.05917      0.10698      0.02107      0.02713      1.00000 |                                          *
     50       0.05519      0.10674      0.01962      0.02740      1.00000 |                                          *
     60       0.05272      0.10717      0.01883      0.02766      1.00000 |                                          *
     70       0.04973      0.10794      0.01883      0.02792      1.00000 |                                           *
     80       0.04767      0.10837      0.01725      0.02898      1.00000 |                                           *
     90       0.04647      0.10906      0.01686      0.02924      1.00000 |                                           *
    100       0.04529      0.10959      0.01646      0.02924      1.00000 |                                           *
    110       0.04409      0.11030      0.01580      0.03056      1.00000 |                                            *
    120       0.04383      0.11066      0.01554      0.03003      1.00000 |                                            *
    130       0.04305      0.11133      0.01514      0.03082      1.00000 |                                            *
    140       0.04181      0.11145      0.01475      0.03082      1.00000 |                                            *
    150       0.04101      0.11185      0.01435      0.03135      1.00000 |                                            *
    160       0.03993      0.11209      0.01396      0.03109      1.00000 |                                            *
    170       0.03859      0.11277      0.01317      0.03056      1.00000 |                                             *
    180       0.03814      0.11296      0.01304      0.03056      1.00000 |                                             *
    190       0.03744      0.11353      0.01277      0.03082      1.00000 |                                             *
    200       0.03640      0.11367      0.01238      0.03082      1.00000 |                                             *
    210       0.03585      0.11458      0.01211      0.03082      1.00000 |                                             *
    220       0.03499      0.11527      0.01159      0.03082      1.00000 |                                              *
    230       0.03411      0.11559      0.01106      0.03082      1.00000 |                                              *
    240       0.03303      0.11620      0.01040      0.03135      1.00000 |                                              *
    250       0.03235      0.11683      0.00988      0.03135      1.00000 |                                              *
    260       0.03155      0.11712      0.00961      0.03135      1.00000 |                                              *
    270       0.03090      0.11762      0.00961      0.03135      1.00000 |                                               *
    280       0.02944      0.11817      0.00935      0.03109      1.00000 |                                               *
    290       0.02874      0.11915      0.00948      0.03161      1.00000 |                                               *
    300       0.02795      0.11981      0.00843      0.03135      1.00000 |                                               *
    310       0.02729      0.12014      0.00830      0.03135      1.00000 |                                               *
    320       0.02664      0.12069      0.00777      0.03161      1.00000 |                                               *
    330       0.02592      0.12160      0.00737      0.03135      1.00000 |                                               *
    340       0.02505      0.12233      0.00685      0.03109      1.00000 |                                               *
    350       0.02445      0.12314      0.00645      0.03135      1.00000 |                                               *
    360       0.02386      0.12376      0.00619      0.03161      1.00000 |                                               *
    370       0.02314      0.12422      0.00540      0.03188      1.00000 |                                               *
    380       0.02242      0.12503      0.00540      0.03188      1.00000 |                                               *
    390       0.02186      0.12588      0.00487      0.03161      1.00000 |                                               *
    400       0.02150      0.12648      0.00487      0.03161      1.00000 |                                               *

 Core TN model building:          2.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    400      1    400      1      3      1.06      6.94  CHILDYRS
    274      1    400      1      7      4.64      2.31  AGE
    193      1    396      1      7      5.04      1.43  INCOME
    190      1    399      1      7      5.29      1.29  AGE_MOM
    184      1    396      2      7      5.28      1.25  EDUC_MOM
    183      1    396      1      7      5.36      1.21  OTH_CHLD
     92      1    400      2      7      5.63      0.55  ILLEGIT
     91      3    400      2      7      5.53      0.56  RACE_MOM
     71      1    400      3      7      5.46      0.45  PNCLATE
     45      1    396      2      7      5.31      0.30  LBW

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     8 terminal nodes
    Largest :    76 terminal nodes
    Average :     20.47250 terminal nodes

 Reconciling 7594 Learn sample scores across 5 selected models,
 the largest having 76 trees, to compute gains and PS tables.

 Reconciling 3796 Test sample scores across 5 selected models,
 the largest having 76 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 17

 Target: ANY_INJ_DTH
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     7594      7594.00
 N Test  Obs:     3796      3796.00
 Learn Rate :    0.1000000

 Storage requirements: 15978 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL           17      0.10534
                  ROC           76      0.84742
                 Lift            4      5.18182
              KS-stat           83      0.60820
          Class.Error           16      0.02634

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.11210     0.11854     0.92115     0.80481     7.19954     4.55636     0.68901     0.56341     0.02910     0.02898
      4     0.09528     0.11079     0.93858     0.82151     7.86672     5.18182     0.73638     0.55067     0.02910     0.02898
     10     0.08014     0.10634     0.95509     0.84158     8.46154     5.09091     0.78545     0.55502     0.02792     0.02766
     16     0.07161     0.10534     0.96565     0.84260     8.73303     4.90909     0.80910     0.59328     0.02555     0.02634
     17     0.07036     0.10534     0.96759     0.84244     8.86878     4.81818     0.81253     0.59056     0.02515     0.02661
     20     0.06806     0.10596     0.97009     0.83745     8.82353     4.90909     0.82055     0.58662     0.02502     0.02687
     30     0.06235     0.10625     0.97718     0.83888     9.09502     5.09091     0.85088     0.57727     0.02291     0.02740
     40     0.05917     0.10698     0.98207     0.83855     9.27602     5.00000     0.87147     0.57714     0.02107     0.02713
     50     0.05519     0.10674     0.98522     0.84258     9.52036     5.00000     0.88029     0.60305     0.01962     0.02740
     60     0.05272     0.10717     0.98718     0.84390     9.59276     4.87273     0.88956     0.58786     0.01883     0.02766
     70     0.04973     0.10794     0.98988     0.84543     9.68326     5.00000     0.90322     0.58853     0.01883     0.02792
     76     0.04815     0.10814     0.99110     0.84742     9.72851     5.09091     0.90502     0.59694     0.01738     0.02766
     80     0.04767     0.10837     0.99144     0.84648     9.81900     5.09091     0.90979     0.59762     0.01725     0.02898
     83     0.04751     0.10856     0.99149     0.84635     9.81900     5.05455     0.90956     0.60820     0.01725     0.02924
     90     0.04647     0.10906     0.99201     0.84558     9.86425     5.00000     0.91676     0.60712     0.01686     0.02924
    100     0.04529     0.10959     0.99257     0.84432     9.90950     4.90909     0.91875     0.59464     0.01646     0.02924
    110     0.04409     0.11030     0.99311     0.84309     9.90950     4.90909     0.92300     0.58771     0.01580     0.03056
    120     0.04383     0.11066     0.99322     0.84177     9.90950     4.72727     0.92314     0.58106     0.01554     0.03003
    130     0.04305     0.11133     0.99363     0.84023     9.90950     4.81818     0.92815     0.58527     0.01514     0.03082
    140     0.04181     0.11145     0.99438     0.84091     9.90950     4.81818     0.92965     0.57415     0.01475     0.03082
    150     0.04101     0.11185     0.99470     0.84013     9.90950     4.81818     0.93005     0.57456     0.01435     0.03135
    160     0.03993     0.11209     0.99517     0.83996     9.90950     4.81818     0.93304     0.57211     0.01396     0.03109
    170     0.03859     0.11277     0.99596     0.83813     9.95475     4.72727     0.93960     0.57537     0.01317     0.03056
    180     0.03814     0.11296     0.99618     0.83798     9.95475     4.63636     0.94022     0.56818     0.01304     0.03056
    190     0.03744     0.11353     0.99644     0.83685     9.95475     4.72727     0.94180     0.56370     0.01277     0.03082
    200     0.03640     0.11367     0.99682     0.83640     9.95475     4.72727     0.94736     0.55407     0.01238     0.03082
    210     0.03585     0.11458     0.99695     0.83258     9.95475     4.63636     0.94831     0.54146     0.01211     0.03082
    215     0.03528     0.11505     0.99707     0.83097    10.00000     4.63636     0.94940     0.54485     0.01172     0.03109
    220     0.03499     0.11527     0.99717     0.83083    10.00000     4.63636     0.95008     0.55244     0.01159     0.03082
    230     0.03411     0.11559     0.99734     0.83096    10.00000     4.63636     0.95075     0.54322     0.01106     0.03082
    240     0.03303     0.11620     0.99767     0.82884    10.00000     4.63636     0.95265     0.53358     0.01040     0.03135
    250     0.03235     0.11683     0.99782     0.82752    10.00000     4.63636     0.95333     0.53358     0.00988     0.03135
    260     0.03155     0.11712     0.99813     0.82716    10.00000     4.54545     0.96455     0.53195     0.00961     0.03135
    270     0.03090     0.11762     0.99823     0.82657    10.00000     4.54545     0.96523     0.52802     0.00961     0.03135
    280     0.02944     0.11817     0.99885     0.82497    10.00000     4.54545     0.97409     0.52028     0.00935     0.03109
    290     0.02874     0.11915     0.99897     0.82321    10.00000     4.54545     0.97437     0.52245     0.00948     0.03161
    300     0.02795     0.11981     0.99912     0.82153    10.00000     4.54545     0.97572     0.51607     0.00843     0.03135
    310     0.02729     0.12014     0.99928     0.82088    10.00000     4.54545     0.97684     0.50739     0.00830     0.03135
    320     0.02664     0.12069     0.99936     0.82023    10.00000     4.45455     0.97893     0.51172     0.00777     0.03161
    330     0.02592     0.12160     0.99946     0.81877    10.00000     4.36364     0.97906     0.51444     0.00737     0.03135
    340     0.02505     0.12233     0.99954     0.81527    10.00000     4.45455     0.98042     0.51824     0.00685     0.03109
    350     0.02445     0.12314     0.99960     0.81438    10.00000     4.36364     0.98159     0.52095     0.00645     0.03135
    360     0.02386     0.12376     0.99969     0.81334    10.00000     4.27273     0.98720     0.51553     0.00619     0.03161
    370     0.02314     0.12422     0.99972     0.81270    10.00000     4.27273     0.98693     0.51729     0.00540     0.03188
    380     0.02242     0.12503     0.99977     0.81096    10.00000     4.18182     0.98842     0.51145     0.00540     0.03188
    389     0.02189     0.12584     0.99979     0.80997    10.00000     4.27273     0.98869     0.50861     0.00487     0.03161
    390     0.02186     0.12588     0.99979     0.81016    10.00000     4.27273     0.98869     0.50861     0.00487     0.03161
    400     0.02150     0.12648     0.99981     0.80916    10.00000     4.27273     0.98924     0.50793     0.00487     0.03161


 =========================================
 Variable Importance for the 17-tree Model
 =========================================

                   Abs     Rel

 CHILDYRS    100.00000  100.00 |***********|
 AGE_MOM      43.54281   43.54 |*****      |
 AGE          33.20851   33.21 |****       |
 EDUC_MOM     33.01665   33.02 |****       |
 INCOME       31.63147   31.63 |****       |
 OTH_CHLD     31.13963   31.14 |****       |
 ILLEGIT      23.40094   23.40 |***        |
 LBW          11.93870   11.94 |**         |
 RACE_MOM     11.72872   11.73 |**         |
 PNCLATE       9.87764    9.88 |**         |


 Learn Sample Misclassification by Target Class
 For The 17-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 7373.00      7367.00         6.00       0.0008
 1                  221.00        36.00       185.00       0.8371


 Test Sample Misclassification by Target Class
 For The 17-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 3686.00      3682.00         4.00       0.0011
 1                  110.00        13.00        97.00       0.8818

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                2.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               2.000 sec ( 0.00 hrs, 100.00%)
    Core model:         2.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             0.000 sec ( 0.00 hrs,   0.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000000 hrs  0.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 753 kb , 76% compression

 Grove file created containing:
      1 TreeNet

