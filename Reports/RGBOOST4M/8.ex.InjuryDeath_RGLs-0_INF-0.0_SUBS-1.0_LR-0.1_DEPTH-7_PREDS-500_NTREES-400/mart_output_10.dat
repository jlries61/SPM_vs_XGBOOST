
 The "USE "../Data/Classification/8.ex.InjuryDeath/SAMPL" command: 00:00:00

 Model (target and predictors) reset: ANY_INJ_DTH

 The KEEP list has 10 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 7594
 Records Kept in Learning sample: 7594

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 3796
 Records Kept in Test sample: 3796

 Discrete         N Levels
 Variable         in Model
 -------------------------
 ANY_INJ_DTH             2

 ======================
 Target Frequency Table
 ======================

 Variable: ANY_INJ_DTH
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         7373  97.09              7373.00  97.09
                 T        (3686  97.10)            (3686.00  97.10)
 1               L          221   2.91               221.00   2.91
                 T         (110   2.90)             (110.00   2.90)
 -----------------------------------------------------------------
 Totals
 0                        11059  97.09             11059.00  97.09
 1                          331   2.91               331.00   2.91
 -----------------------------------------------------------------
 Total                    11390                    11390.00
 Total Learn               7594                     7594.00
 Total Test                3796                     3796.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.11246      0.11956      0.02910      0.02898      1.00000 |                                               *
      2       0.10481      0.11714      0.02910      0.02898      1.00000 |                                              *
      3       0.09900      0.11546      0.02910      0.02898      1.00000 |                                             *
      4       0.09471      0.11434      0.02910      0.02898      1.00000 |                                             *
      5       0.09071      0.11317      0.02910      0.02898      1.00000 |                                            *
      6       0.08728      0.11259      0.02910      0.02898      1.00000 |                                            *
      7       0.08473      0.11213      0.02910      0.02898      1.00000 |                                            *
      8       0.08216      0.11169      0.02831      0.02845      1.00000 |                                            *
      9       0.07896      0.11131      0.02779      0.02819      1.00000 |                                            *
     10       0.07631      0.11113      0.02660      0.02819      1.00000 |                                            *
     11       0.07414      0.11119      0.02607      0.02845      1.00000 |                                            *
     12       0.07207      0.11151      0.02594      0.02871      1.00000 |                                            *
     13       0.07097      0.11212      0.02568      0.02871      1.00000 |                                            *
     14       0.06986      0.11227      0.02568      0.02898      1.00000 |                                            *
     15       0.06895      0.11266      0.02541      0.02924      1.00000 |                                            *
     16       0.06818      0.11304      0.02515      0.02924      1.00000 |                                            *
     17       0.06685      0.11313      0.02449      0.02924      1.00000 |                                            *
     18       0.06634      0.11333      0.02410      0.02924      1.00000 |                                             *
     19       0.06513      0.11323      0.02383      0.02924      1.00000 |                                            *
     20       0.06415      0.11341      0.02357      0.02950      1.00000 |                                             *
     30       0.06002      0.11558      0.02094      0.03109      1.00000 |                                             *
     40       0.05406      0.11678      0.01962      0.03082      1.00000 |                                              *
     50       0.05107      0.11820      0.01844      0.03109      1.00000 |                                              *
     60       0.04860      0.11952      0.01738      0.03135      1.00000 |                                               *
     70       0.04578      0.12170      0.01580      0.03188      1.00000 |                                               *
     80       0.04417      0.12283      0.01514      0.03267      1.00000 |                                               *
     90       0.04218      0.12418      0.01488      0.03267      1.00000 |                                               *
    100       0.04093      0.12482      0.01462      0.03293      1.00000 |                                               *
    110       0.04004      0.12571      0.01435      0.03293      1.00000 |                                               *
    120       0.03936      0.12628      0.01356      0.03319      1.00000 |                                               *
    130       0.03853      0.12712      0.01356      0.03267      1.00000 |                                               *
    140       0.03788      0.12779      0.01304      0.03293      1.00000 |                                               *
    150       0.03767      0.12833      0.01277      0.03319      1.00000 |                                               *
    160       0.03697      0.12914      0.01225      0.03319      1.00000 |                                               *
    170       0.03667      0.12962      0.01238      0.03319      1.00000 |                                               *
    180       0.03562      0.13071      0.01185      0.03267      1.00000 |                                               *
    190       0.03457      0.13140      0.01132      0.03319      1.00000 |                                               *
    200       0.03403      0.13206      0.01106      0.03319      1.00000 |                                               *
    210       0.03349      0.13283      0.01067      0.03346      1.00000 |                                               *
    220       0.03258      0.13357      0.01053      0.03319      1.00000 |                                               *
    230       0.03169      0.13478      0.01027      0.03319      1.00000 |                                               *
    240       0.03110      0.13576      0.01014      0.03319      1.00000 |                                               *
    250       0.03044      0.13661      0.00961      0.03319      1.00000 |                                               *
    260       0.02921      0.13804      0.00948      0.03319      1.00000 |                                               *
    270       0.02791      0.13851      0.00843      0.03319      1.00000 |                                               *
    280       0.02727      0.13931      0.00803      0.03293      1.00000 |                                               *
    290       0.02682      0.14036      0.00790      0.03319      1.00000 |                                               *
    300       0.02582      0.14083      0.00724      0.03372      1.00000 |                                               *
    310       0.02533      0.14190      0.00711      0.03372      1.00000 |                                               *
    320       0.02478      0.14253      0.00672      0.03372      1.00000 |                                               *
    330       0.02432      0.14317      0.00658      0.03398      1.00000 |                                               *
    340       0.02340      0.14400      0.00632      0.03372      1.00000 |                                               *
    350       0.02298      0.14474      0.00593      0.03398      1.00000 |                                               *
    360       0.02217      0.14568      0.00566      0.03398      1.00000 |                                               *
    370       0.02110      0.14664      0.00540      0.03425      1.00000 |                                               *
    380       0.02062      0.14730      0.00474      0.03425      1.00000 |                                               *
    390       0.01977      0.14847      0.00435      0.03398      1.00000 |                                               *
    400       0.01931      0.14947      0.00421      0.03398      1.00000 |                                               *

 Core TN model building:          1.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    400      1    400      1      2      1.05      6.95  CHILDYRS
    303      1    400      1      7      4.78      2.44  AGE
    253      1    400      1      7      5.11      1.83  INCOME
    217      1    400      2      7      5.33      1.45  EDUC_MOM
    211      1    399      1      7      5.02      1.57  OTH_CHLD
    196      1    400      2      7      5.61      1.17  AGE_MOM
    136      1    399      1      7      5.37      0.90  ILLEGIT
     80      1    400      3      7      5.81      0.44  RACE_MOM
     57      1    400      4      7      5.77      0.32  PNCLATE
     54      1    388      4      7      5.57      0.33  LBW

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     8 terminal nodes
    Largest :    77 terminal nodes
    Average :     21.02250 terminal nodes

 Reconciling 7594 Learn sample scores across 5 selected models,
 the largest having 10 trees, to compute gains and PS tables.

 Reconciling 3796 Test sample scores across 5 selected models,
 the largest having 10 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 10

 Target: ANY_INJ_DTH
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     7594      7594.00
 N Test  Obs:     3796      3796.00
 Learn Rate :    0.1000000

 Storage requirements: 16418 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL           10      0.11113
                  ROC           10      0.82002
                 Lift            3      4.54545
              KS-stat            8      0.54729
          Class.Error            9      0.02819

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.11246     0.11956     0.92871     0.77571     7.45249     4.44596     0.71343     0.48431     0.02910     0.02898
      3     0.09900     0.11546     0.94911     0.79625     8.09955     4.54545     0.77628     0.48839     0.02910     0.02898
      8     0.08216     0.11169     0.96047     0.81474     8.44947     4.36364     0.80566     0.54729     0.02831     0.02845
      9     0.07896     0.11131     0.96521     0.81805     8.59729     4.18182     0.81385     0.53467     0.02779     0.02819
     10     0.07631     0.11113     0.96754     0.82002     8.68778     4.27273     0.82331     0.52844     0.02660     0.02819
     20     0.06415     0.11341     0.97377     0.81244     9.14027     4.41818     0.84556     0.50482     0.02357     0.02950
     30     0.06002     0.11558     0.97592     0.80761     9.27602     4.41818     0.85960     0.49953     0.02094     0.03109
     40     0.05406     0.11678     0.98514     0.80411     9.63801     4.27273     0.90543     0.51608     0.01962     0.03082
     50     0.05107     0.11820     0.98880     0.80077     9.68326     4.27273     0.91484     0.50455     0.01844     0.03109
     60     0.04860     0.11952     0.99069     0.80009     9.72851     4.18182     0.92430     0.48852     0.01738     0.03135
     70     0.04578     0.12170     0.99286     0.79467     9.77376     3.81818     0.93067     0.46967     0.01580     0.03188
     80     0.04417     0.12283     0.99359     0.79336     9.77376     3.72727     0.93081     0.46860     0.01514     0.03267
     90     0.04218     0.12418     0.99453     0.79017     9.81900     3.72727     0.93293     0.47864     0.01488     0.03267
    100     0.04093     0.12482     0.99508     0.78952     9.90950     3.72727     0.93347     0.47117     0.01462     0.03293
    110     0.04004     0.12571     0.99539     0.78913     9.90950     3.72727     0.93550     0.46575     0.01435     0.03293
    120     0.03936     0.12628     0.99566     0.78816     9.90950     3.63636     0.93632     0.46343     0.01356     0.03319
    130     0.03853     0.12712     0.99588     0.78656     9.90950     3.72727     0.94039     0.46112     0.01356     0.03267
    140     0.03788     0.12779     0.99611     0.78542     9.90950     3.81818     0.94740     0.46017     0.01304     0.03293
    150     0.03767     0.12833     0.99616     0.78467     9.90950     3.72727     0.94767     0.46044     0.01277     0.03319
    160     0.03697     0.12914     0.99637     0.78296     9.90950     3.54545     0.94740     0.46454     0.01225     0.03319
    170     0.03667     0.12962     0.99647     0.78265     9.90950     3.45455     0.94916     0.46726     0.01238     0.03319
    180     0.03562     0.13071     0.99669     0.78179     9.90950     3.54545     0.95188     0.45504     0.01185     0.03267
    185     0.03486     0.13102     0.99693     0.78225    10.00000     3.63636     0.95174     0.45599     0.01159     0.03293
    190     0.03457     0.13140     0.99701     0.78092    10.00000     3.54545     0.95228     0.45613     0.01132     0.03319
    200     0.03403     0.13206     0.99715     0.77973    10.00000     3.36364     0.95188     0.46155     0.01106     0.03319
    210     0.03349     0.13283     0.99730     0.77732     9.97285     3.45455     0.95283     0.45694     0.01067     0.03346
    220     0.03258     0.13357     0.99766     0.77631    10.00000     3.50909     0.95378     0.46562     0.01053     0.03319
    230     0.03169     0.13478     0.99787     0.77170    10.00000     3.36364     0.95364     0.44541     0.01027     0.03319
    240     0.03110     0.13576     0.99795     0.77093    10.00000     3.36364     0.95522     0.44773     0.01014     0.03319
    250     0.03044     0.13661     0.99804     0.76858    10.00000     3.45455     0.95712     0.43850     0.00961     0.03319
    260     0.02921     0.13804     0.99835     0.76581    10.00000     3.45455     0.95888     0.44568     0.00948     0.03319
    270     0.02791     0.13851     0.99878     0.76743    10.00000     3.36364     0.96440     0.43918     0.00843     0.03319
    280     0.02727     0.13931     0.99893     0.76532    10.00000     3.45455     0.96698     0.44135     0.00803     0.03293
    290     0.02682     0.14036     0.99899     0.76244    10.00000     3.45455     0.96734     0.43497     0.00790     0.03319
    300     0.02582     0.14083     0.99916     0.76323    10.00000     3.45455     0.96793     0.43673     0.00724     0.03372
    310     0.02533     0.14190     0.99927     0.76087    10.00000     3.45455     0.97078     0.44393     0.00711     0.03372
    320     0.02478     0.14253     0.99936     0.76034    10.00000     3.45455     0.97291     0.44582     0.00672     0.03372
    330     0.02432     0.14317     0.99938     0.76037    10.00000     3.45455     0.97400     0.44501     0.00658     0.03398
    340     0.02340     0.14400     0.99947     0.75816    10.00000     3.54545     0.97788     0.43714     0.00632     0.03372
    350     0.02298     0.14474     0.99950     0.75667    10.00000     3.54545     0.97788     0.43334     0.00593     0.03398
    360     0.02217     0.14568     0.99956     0.75543    10.00000     3.45455     0.97969     0.43660     0.00566     0.03398
    370     0.02110     0.14664     0.99969     0.75314    10.00000     3.45455     0.98770     0.42196     0.00540     0.03425
    380     0.02062     0.14730     0.99971     0.75331    10.00000     3.54545     0.98797     0.43404     0.00474     0.03425
    385     0.02004     0.14780     0.99981     0.75400    10.00000     3.72727     0.98837     0.42779     0.00448     0.03425
    390     0.01977     0.14847     0.99983     0.75237    10.00000     3.72727     0.98824     0.42141     0.00435     0.03398
    400     0.01931     0.14947     0.99984     0.74934    10.00000     3.72727     0.98837     0.41978     0.00421     0.03398


 =========================================
 Variable Importance for the 10-tree Model
 =========================================

                   Abs     Rel

 CHILDYRS    100.00000  100.00 |***********|
 EDUC_MOM     39.18734   39.19 |*****      |
 AGE_MOM      35.74552   35.75 |*****      |
 OTH_CHLD     34.43246   34.43 |****       |
 AGE          32.94771   32.95 |****       |
 INCOME       31.22269   31.22 |****       |
 ILLEGIT      23.58687   23.59 |***        |
 RACE_MOM     19.37247   19.37 |***        |
 PNCLATE      14.93663   14.94 |**         |
 LBW           8.11081    8.11 |**         |


 Learn Sample Misclassification by Target Class
 For The 10-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 7373.00      7370.00         3.00       0.0004
 1                  221.00        22.00       199.00       0.9005


 Test Sample Misclassification by Target Class
 For The 10-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 3686.00      3680.00         6.00       0.0016
 1                  110.00         9.00       101.00       0.9182

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                1.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               1.000 sec ( 0.00 hrs, 100.00%)
    Core model:         1.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             0.000 sec ( 0.00 hrs,   0.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000000 hrs  0.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 760 kb , 76% compression

 Grove file created containing:
      1 TreeNet

