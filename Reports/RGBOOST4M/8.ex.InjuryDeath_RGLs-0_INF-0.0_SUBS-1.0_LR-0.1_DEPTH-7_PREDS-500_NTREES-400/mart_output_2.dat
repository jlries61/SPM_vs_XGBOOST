
 The "USE "../Data/Classification/8.ex.InjuryDeath/SAMPL" command: 00:00:00

 Model (target and predictors) reset: ANY_INJ_DTH

 The KEEP list has 10 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 7594
 Records Kept in Learning sample: 7594

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 3796
 Records Kept in Test sample: 3796

 Discrete         N Levels
 Variable         in Model
 -------------------------
 ANY_INJ_DTH             2

 ======================
 Target Frequency Table
 ======================

 Variable: ANY_INJ_DTH
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         7373  97.09              7373.00  97.09
                 T        (3686  97.10)            (3686.00  97.10)
 1               L          221   2.91               221.00   2.91
                 T         (110   2.90)             (110.00   2.90)
 -----------------------------------------------------------------
 Totals
 0                        11059  97.09             11059.00  97.09
 1                          331   2.91               331.00   2.91
 -----------------------------------------------------------------
 Total                    11390                    11390.00
 Total Learn               7594                     7594.00
 Total Test                3796                     3796.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.11136      0.12073      0.02910      0.02898      1.00000 |                                               *
      2       0.10285      0.11666      0.02910      0.02898      1.00000 |                                             *
      3       0.09773      0.11407      0.02910      0.02898      1.00000 |                                            *
      4       0.09317      0.11226      0.02910      0.02898      1.00000 |                                            *
      5       0.08936      0.11076      0.02910      0.02898      1.00000 |                                           *
      6       0.08623      0.10990      0.02897      0.02898      1.00000 |                                           *
      7       0.08352      0.10903      0.02792      0.02819      1.00000 |                                          *
      8       0.08123      0.10814      0.02792      0.02845      1.00000 |                                          *
      9       0.07879      0.10796      0.02660      0.02845      1.00000 |                                          *
     10       0.07661      0.10743      0.02620      0.02819      1.00000 |                                          *
     11       0.07482      0.10736      0.02555      0.02819      1.00000 |                                          *
     12       0.07307      0.10723      0.02489      0.02792      1.00000 |                                          *
     13       0.07095      0.10716      0.02423      0.02845      1.00000 |                                          *
     14       0.06963      0.10714      0.02370      0.02845      1.00000 |                                          *
     15       0.06809      0.10747      0.02344      0.02819      1.00000 |                                          *
     16       0.06743      0.10756      0.02357      0.02845      1.00000 |                                          *
     17       0.06650      0.10765      0.02344      0.02898      1.00000 |                                          *
     18       0.06570      0.10775      0.02357      0.02898      1.00000 |                                          *
     19       0.06513      0.10798      0.02304      0.02898      1.00000 |                                          *
     20       0.06419      0.10786      0.02278      0.02871      1.00000 |                                          *
     30       0.05993      0.10918      0.02146      0.03056      1.00000 |                                          *
     40       0.05685      0.10931      0.02015      0.03082      1.00000 |                                          *
     50       0.05568      0.10945      0.01975      0.03135      1.00000 |                                           *
     60       0.05402      0.11001      0.01923      0.03082      1.00000 |                                           *
     70       0.05198      0.11032      0.01830      0.03082      1.00000 |                                           *
     80       0.05039      0.11077      0.01765      0.03056      1.00000 |                                           *
     90       0.04885      0.11136      0.01686      0.03056      1.00000 |                                           *
    100       0.04782      0.11181      0.01633      0.03056      1.00000 |                                           *
    110       0.04647      0.11228      0.01633      0.03082      1.00000 |                                            *
    120       0.04565      0.11301      0.01580      0.03109      1.00000 |                                            *
    130       0.04496      0.11329      0.01541      0.03082      1.00000 |                                            *
    140       0.04431      0.11349      0.01528      0.03082      1.00000 |                                            *
    150       0.04298      0.11379      0.01488      0.03161      1.00000 |                                            *
    160       0.04176      0.11455      0.01475      0.03161      1.00000 |                                             *
    170       0.04082      0.11494      0.01435      0.03161      1.00000 |                                             *
    180       0.04050      0.11540      0.01383      0.03161      1.00000 |                                             *
    190       0.03939      0.11605      0.01343      0.03161      1.00000 |                                             *
    200       0.03847      0.11652      0.01304      0.03188      1.00000 |                                             *
    210       0.03721      0.11694      0.01211      0.03161      1.00000 |                                             *
    220       0.03647      0.11778      0.01172      0.03188      1.00000 |                                              *
    230       0.03539      0.11825      0.01106      0.03214      1.00000 |                                              *
    240       0.03481      0.11881      0.01080      0.03214      1.00000 |                                              *
    250       0.03426      0.11920      0.01053      0.03214      1.00000 |                                              *
    260       0.03328      0.11962      0.01014      0.03240      1.00000 |                                               *
    270       0.03278      0.11993      0.00988      0.03214      1.00000 |                                               *
    280       0.03176      0.12078      0.00922      0.03240      1.00000 |                                               *
    290       0.03046      0.12178      0.00856      0.03267      1.00000 |                                               *
    300       0.02971      0.12272      0.00803      0.03319      1.00000 |                                               *
    310       0.02902      0.12369      0.00790      0.03372      1.00000 |                                               *
    320       0.02825      0.12369      0.00764      0.03372      1.00000 |                                               *
    330       0.02734      0.12422      0.00724      0.03293      1.00000 |                                               *
    340       0.02659      0.12523      0.00724      0.03319      1.00000 |                                               *
    350       0.02591      0.12610      0.00698      0.03293      1.00000 |                                               *
    360       0.02428      0.12766      0.00645      0.03293      1.00000 |                                               *
    370       0.02353      0.12877      0.00619      0.03319      1.00000 |                                               *
    380       0.02252      0.12951      0.00593      0.03319      1.00000 |                                               *
    390       0.02145      0.13053      0.00553      0.03293      1.00000 |                                               *
    400       0.02044      0.13229      0.00527      0.03346      1.00000 |                                               *

 Core TN model building:          2.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    400      1    400      1      3      1.22      6.78  CHILDYRS
    221      1    400      1      7      4.27      2.06  AGE
    220      1    400      1      7      4.29      2.04  AGE_MOM
    187      1    399      1      7      4.94      1.43  INCOME
    167      1    398      1      7      4.56      1.44  OTH_CHLD
    150      1    396      1      7      4.84      1.19  EDUC_MOM
     80      2    399      3      7      5.96      0.41  ILLEGIT
     79      1    398      3      7      5.61      0.47  RACE_MOM
     54      1    394      3      7      5.83      0.29  PNCLATE
     34      2    382      4      7      6.12      0.16  LBW

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     8 terminal nodes
    Largest :    78 terminal nodes
    Average :     20.59250 terminal nodes

 Reconciling 7594 Learn sample scores across 5 selected models,
 the largest having 14 trees, to compute gains and PS tables.

 Reconciling 3796 Test sample scores across 5 selected models,
 the largest having 14 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 14

 Target: ANY_INJ_DTH
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     7594      7594.00
 N Test  Obs:     3796      3796.00
 Learn Rate :    0.1000000

 Storage requirements: 16074 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL           14      0.10714
                  ROC            3      0.84168
                 Lift            3      5.09091
              KS-stat            3      0.57958
          Class.Error           12      0.02792

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.11136     0.12073     0.92492     0.79975     7.33937     4.00000     0.69979     0.48071     0.02910     0.02898
      3     0.09773     0.11407     0.94102     0.84168     7.93062     5.09091     0.73059     0.57958     0.02910     0.02898
     10     0.07661     0.10743     0.96085     0.83866     8.50679     5.09091     0.80398     0.56086     0.02620     0.02819
     12     0.07307     0.10723     0.96451     0.83645     8.73303     4.72727     0.81191     0.55803     0.02489     0.02792
     14     0.06963     0.10714     0.96871     0.83635     9.00452     4.81818     0.82889     0.55734     0.02370     0.02845
     20     0.06419     0.10786     0.97279     0.83624     9.04977     5.09091     0.83282     0.53792     0.02278     0.02871
     30     0.05993     0.10918     0.97614     0.83419     9.23077     4.90909     0.85245     0.53059     0.02146     0.03056
     40     0.05685     0.10931     0.98209     0.83950     9.41176     4.90909     0.87078     0.53886     0.02015     0.03082
     50     0.05568     0.10945     0.98286     0.84048     9.41176     4.81818     0.87065     0.53293     0.01975     0.03135
     60     0.05402     0.11001     0.98403     0.83928     9.41176     4.78182     0.87133     0.53401     0.01923     0.03082
     70     0.05198     0.11032     0.98578     0.83642     9.50226     4.81818     0.87919     0.53048     0.01830     0.03082
     80     0.05039     0.11077     0.98701     0.83557     9.54751     4.81818     0.88702     0.53808     0.01765     0.03056
     90     0.04885     0.11136     0.98811     0.83462     9.54751     4.72727     0.88898     0.52883     0.01686     0.03056
    100     0.04782     0.11181     0.98862     0.83211     9.63801     4.81818     0.89575     0.52910     0.01633     0.03056
    110     0.04647     0.11228     0.98984     0.83140     9.63801     4.72727     0.89738     0.53208     0.01633     0.03082
    120     0.04565     0.11301     0.99034     0.82883     9.72851     4.63636     0.90188     0.53303     0.01580     0.03109
    130     0.04496     0.11329     0.99057     0.82880     9.72851     4.63636     0.90449     0.53114     0.01541     0.03082
    140     0.04431     0.11349     0.99080     0.82756     9.72851     4.72727     0.90512     0.52733     0.01528     0.03082
    150     0.04298     0.11379     0.99188     0.82710     9.77376     4.72727     0.91959     0.53045     0.01488     0.03161
    160     0.04176     0.11455     0.99258     0.82304     9.81900     4.72727     0.92587     0.51771     0.01475     0.03161
    170     0.04082     0.11494     0.99312     0.82380     9.81900     4.72727     0.93302     0.52232     0.01435     0.03161
    180     0.04050     0.11540     0.99325     0.82155     9.81900     4.72727     0.93370     0.52178     0.01383     0.03161
    190     0.03939     0.11605     0.99393     0.81969     9.81900     4.72727     0.94297     0.52544     0.01343     0.03161
    200     0.03847     0.11652     0.99422     0.81866     9.81900     4.72727     0.94311     0.50957     0.01304     0.03188
    210     0.03721     0.11694     0.99544     0.81762     9.90950     4.63636     0.94650     0.51960     0.01211     0.03161
    220     0.03647     0.11778     0.99560     0.81393     9.90950     4.54545     0.94840     0.52421     0.01172     0.03188
    230     0.03539     0.11825     0.99626     0.81399     9.90950     4.63636     0.95957     0.51865     0.01106     0.03214
    240     0.03481     0.11881     0.99640     0.81252     9.90950     4.63636     0.96011     0.51702     0.01080     0.03214
    250     0.03426     0.11920     0.99653     0.81269     9.90950     4.54545     0.96025     0.51621     0.01053     0.03214
    260     0.03328     0.11962     0.99677     0.81254     9.90950     4.60000     0.96038     0.50807     0.01014     0.03240
    270     0.03278     0.11993     0.99687     0.81080     9.90950     4.63636     0.96133     0.50075     0.00988     0.03214
    280     0.03176     0.12078     0.99720     0.80932     9.90950     4.54545     0.96283     0.49857     0.00922     0.03240
    290     0.03046     0.12178     0.99802     0.80761     9.95475     4.63636     0.96586     0.50604     0.00856     0.03267
    300     0.02971     0.12272     0.99822     0.80507     9.95475     4.54545     0.96708     0.50469     0.00803     0.03319
    310     0.02902     0.12369     0.99837     0.80133     9.95475     4.54545     0.96893     0.49641     0.00790     0.03372
    320     0.02825     0.12369     0.99858     0.80295     9.95475     4.54545     0.97156     0.50388     0.00764     0.03372
    330     0.02734     0.12422     0.99876     0.80084     9.95475     4.45455     0.97183     0.50035     0.00724     0.03293
    340     0.02659     0.12523     0.99885     0.79752     9.95475     4.45455     0.97390     0.49425     0.00724     0.03319
    350     0.02591     0.12610     0.99893     0.79557     9.95475     4.36364     0.97498     0.49275     0.00698     0.03293
    357     0.02482     0.12721     0.99943     0.79337    10.00000     4.36364     0.97620     0.49342     0.00685     0.03293
    360     0.02428     0.12766     0.99948     0.79218    10.00000     4.45455     0.97620     0.48855     0.00645     0.03293
    370     0.02353     0.12877     0.99952     0.78918    10.00000     4.45455     0.97593     0.48705     0.00619     0.03319
    380     0.02252     0.12951     0.99967     0.78995    10.00000     4.45455     0.98020     0.49519     0.00593     0.03319
    390     0.02145     0.13053     0.99979     0.78700    10.00000     4.54545     0.98517     0.48760     0.00553     0.03293
    398     0.02048     0.13200     0.99985     0.78338    10.00000     4.45455     0.98829     0.47281     0.00527     0.03319
    399     0.02047     0.13211     0.99985     0.78294    10.00000     4.45455     0.98834     0.47335     0.00527     0.03319
    400     0.02044     0.13229     0.99985     0.78197    10.00000     4.45455     0.98829     0.47363     0.00527     0.03346


 =========================================
 Variable Importance for the 14-tree Model
 =========================================

                   Abs     Rel

 CHILDYRS    100.00000  100.00 |***********|
 AGE          43.76565   43.77 |*****      |
 AGE_MOM      42.06002   42.06 |*****      |
 INCOME       33.55643   33.56 |****       |
 OTH_CHLD     32.18560   32.19 |****       |
 EDUC_MOM     24.83163   24.83 |***        |
 PNCLATE      19.22162   19.22 |***        |
 RACE_MOM     16.83659   16.84 |***        |
 ILLEGIT      16.12365   16.12 |***        |
 LBW          10.55603   10.56 |**         |


 Learn Sample Misclassification by Target Class
 For The 14-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 7373.00      7370.00         3.00       0.0004
 1                  221.00        44.00       177.00       0.8009


 Test Sample Misclassification by Target Class
 For The 14-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 3686.00      3680.00         6.00       0.0016
 1                  110.00         8.00       102.00       0.9273

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                2.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               2.000 sec ( 0.00 hrs, 100.00%)
    Core model:         2.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             0.000 sec ( 0.00 hrs,   0.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000000 hrs  0.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 758 kb , 76% compression

 Grove file created containing:
      1 TreeNet

