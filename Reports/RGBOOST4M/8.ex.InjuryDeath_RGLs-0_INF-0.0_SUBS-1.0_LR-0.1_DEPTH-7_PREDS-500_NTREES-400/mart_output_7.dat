
 The "USE "../Data/Classification/8.ex.InjuryDeath/SAMPL" command: 00:00:00

 Model (target and predictors) reset: ANY_INJ_DTH

 The KEEP list has 10 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 7594
 Records Kept in Learning sample: 7594

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 3796
 Records Kept in Test sample: 3796

 Discrete         N Levels
 Variable         in Model
 -------------------------
 ANY_INJ_DTH             2

 ======================
 Target Frequency Table
 ======================

 Variable: ANY_INJ_DTH
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         7373  97.09              7373.00  97.09
                 T        (3686  97.10)            (3686.00  97.10)
 1               L          221   2.91               221.00   2.91
                 T         (110   2.90)             (110.00   2.90)
 -----------------------------------------------------------------
 Totals
 0                        11059  97.09             11059.00  97.09
 1                          331   2.91               331.00   2.91
 -----------------------------------------------------------------
 Total                    11390                    11390.00
 Total Learn               7594                     7594.00
 Total Test                3796                     3796.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.11155      0.11675      0.02910      0.02898      1.00000 |                                               *
      2       0.10529      0.11241      0.02910      0.02898      1.00000 |                                             *
      3       0.10057      0.10978      0.02910      0.02898      1.00000 |                                            *
      4       0.09583      0.10862      0.02910      0.02898      1.00000 |                                            *
      5       0.09257      0.10667      0.02910      0.02898      1.00000 |                                           *
      6       0.08910      0.10495      0.02910      0.02898      1.00000 |                                          *
      7       0.08658      0.10387      0.02884      0.02845      1.00000 |                                          *
      8       0.08430      0.10299      0.02792      0.02766      1.00000 |                                         *
      9       0.08215      0.10268      0.02726      0.02792      1.00000 |                                         *
     10       0.08024      0.10203      0.02581      0.02766      1.00000 |                                         *
     11       0.07838      0.10205      0.02515      0.02792      1.00000 |                                         *
     12       0.07650      0.10158      0.02515      0.02845      1.00000 |                                         *
     13       0.07445      0.10118      0.02489      0.02819      1.00000 |                                         *
     14       0.07300      0.10107      0.02436      0.02766      1.00000 |                                         *
     15       0.07235      0.10065      0.02397      0.02740      1.00000 |                                        *
     16       0.07118      0.10038      0.02383      0.02713      1.00000 |                                        *
     17       0.07068      0.10024      0.02383      0.02766      1.00000 |                                        *
     18       0.06973      0.09994      0.02370      0.02766      1.00000 |                                        *
     19       0.06847      0.09966      0.02344      0.02792      1.00000 |                                        *
     20       0.06730      0.09958      0.02291      0.02766      1.00000 |                                        *
     30       0.06201      0.09905      0.02120      0.02766      1.00000 |                                        *
     40       0.05742      0.09886      0.02015      0.02766      1.00000 |                                        *
     50       0.05452      0.09899      0.01936      0.02740      1.00000 |                                        *
     60       0.05298      0.09874      0.01844      0.02713      1.00000 |                                        *
     70       0.05096      0.09892      0.01830      0.02713      1.00000 |                                        *
     80       0.05029      0.09901      0.01830      0.02740      1.00000 |                                        *
     90       0.04787      0.09959      0.01738      0.02740      1.00000 |                                        *
    100       0.04684      0.09973      0.01686      0.02766      1.00000 |                                        *
    110       0.04541      0.10006      0.01593      0.02766      1.00000 |                                        *
    120       0.04441      0.10017      0.01580      0.02713      1.00000 |                                        *
    130       0.04422      0.10048      0.01580      0.02740      1.00000 |                                        *
    140       0.04327      0.10099      0.01514      0.02792      1.00000 |                                         *
    150       0.04216      0.10130      0.01462      0.02766      1.00000 |                                         *
    160       0.04016      0.10218      0.01370      0.02766      1.00000 |                                         *
    170       0.03825      0.10230      0.01343      0.02766      1.00000 |                                         *
    180       0.03692      0.10272      0.01304      0.02740      1.00000 |                                         *
    190       0.03550      0.10347      0.01225      0.02792      1.00000 |                                          *
    200       0.03411      0.10396      0.01159      0.02819      1.00000 |                                          *
    210       0.03336      0.10460      0.01093      0.02845      1.00000 |                                          *
    220       0.03271      0.10506      0.01053      0.02845      1.00000 |                                          *
    230       0.03151      0.10584      0.00988      0.02845      1.00000 |                                           *
    240       0.03061      0.10605      0.00935      0.02819      1.00000 |                                           *
    250       0.02908      0.10717      0.00869      0.02871      1.00000 |                                           *
    260       0.02836      0.10776      0.00843      0.02898      1.00000 |                                           *
    270       0.02774      0.10817      0.00790      0.02898      1.00000 |                                           *
    280       0.02714      0.10888      0.00764      0.02924      1.00000 |                                            *
    290       0.02659      0.10929      0.00711      0.02898      1.00000 |                                            *
    300       0.02576      0.10997      0.00658      0.02898      1.00000 |                                            *
    310       0.02469      0.11043      0.00645      0.02950      1.00000 |                                            *
    320       0.02415      0.11085      0.00632      0.02924      1.00000 |                                             *
    330       0.02388      0.11128      0.00606      0.02924      1.00000 |                                             *
    340       0.02327      0.11182      0.00593      0.02977      1.00000 |                                             *
    350       0.02267      0.11229      0.00593      0.02977      1.00000 |                                             *
    360       0.02206      0.11278      0.00579      0.02871      1.00000 |                                             *
    370       0.02144      0.11330      0.00566      0.02924      1.00000 |                                              *
    380       0.02089      0.11374      0.00566      0.02898      1.00000 |                                              *
    390       0.02079      0.11391      0.00566      0.02898      1.00000 |                                              *
    400       0.02046      0.11433      0.00540      0.02898      1.00000 |                                              *

 Core TN model building:          2.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    400      1    400      1      3      1.06      6.94  CHILDYRS
    231      1    400      1      7      5.04      1.71  AGE_MOM
    225      1    397      1      7      5.09      1.64  AGE
    223      1    396      2      7      5.22      1.55  INCOME
    202      1    394      1      7      5.17      1.43  OTH_CHLD
    189      1    392      1      7      4.78      1.52  EDUC_MOM
    126      1    400      1      7      5.56      0.77  ILLEGIT
     87      1    396      3      7      5.77      0.49  RACE_MOM
     73      2    396      3      7      5.42      0.47  PNCLATE
     51      4    392      1      7      5.49      0.32  LBW

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     8 terminal nodes
    Largest :    71 terminal nodes
    Average :     21.04500 terminal nodes

 Reconciling 7594 Learn sample scores across 5 selected models,
 the largest having 66 trees, to compute gains and PS tables.

 Reconciling 3796 Test sample scores across 5 selected models,
 the largest having 66 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 61

 Target: ANY_INJ_DTH
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     7594      7594.00
 N Test  Obs:     3796      3796.00
 Learn Rate :    0.1000000

 Storage requirements: 16436 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL           61      0.09852
                  ROC           10      0.86983
                 Lift           27      6.09091
              KS-stat           13      0.61173
          Class.Error           66      0.02687

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.11155     0.11675     0.91673     0.74861     7.16373     5.29761     0.67309     0.45036     0.02910     0.02898
     10     0.08024     0.10203     0.94891     0.86983     8.25339     5.81818     0.76453     0.60101     0.02581     0.02766
     13     0.07445     0.10118     0.95813     0.86702     8.59729     5.72727     0.79895     0.61173     0.02489     0.02819
     20     0.06730     0.09958     0.97026     0.86201     8.95928     5.72727     0.83071     0.59302     0.02291     0.02766
     27     0.06305     0.09888     0.97350     0.86355     9.00452     6.09091     0.83086     0.59925     0.02146     0.02766
     30     0.06201     0.09905     0.97413     0.86080     9.09502     5.90909     0.83759     0.59002     0.02120     0.02766
     40     0.05742     0.09886     0.98063     0.86014     9.32127     5.90909     0.86239     0.59070     0.02015     0.02766
     50     0.05452     0.09899     0.98436     0.86091     9.59276     5.81818     0.89014     0.60262     0.01936     0.02740
     60     0.05298     0.09874     0.98544     0.86250     9.63801     5.90909     0.89097     0.58880     0.01844     0.02713
     61     0.05228     0.09852     0.98615     0.86374     9.63801     5.81818     0.89527     0.60196     0.01830     0.02713
     66     0.05171     0.09880     0.98647     0.86340     9.63801     5.90909     0.89663     0.60034     0.01857     0.02687
     70     0.05096     0.09892     0.98719     0.86353     9.68326     5.72727     0.89788     0.59003     0.01830     0.02713
     80     0.05029     0.09901     0.98751     0.86333     9.68326     5.81818     0.90109     0.58470     0.01830     0.02740
     90     0.04787     0.09959     0.99049     0.86084     9.68326     5.63636     0.90684     0.58350     0.01738     0.02740
    100     0.04684     0.09973     0.99106     0.86102     9.68326     5.63636     0.90955     0.58526     0.01686     0.02766
    110     0.04541     0.10006     0.99221     0.85870     9.77376     5.72727     0.91846     0.57658     0.01593     0.02766
    120     0.04441     0.10017     0.99283     0.85855     9.77376     5.72727     0.91643     0.58066     0.01580     0.02713
    130     0.04422     0.10048     0.99292     0.85671     9.77376     5.72727     0.91656     0.57130     0.01580     0.02740
    140     0.04327     0.10099     0.99351     0.85552     9.81900     5.63636     0.92470     0.57184     0.01514     0.02792
    150     0.04216     0.10130     0.99404     0.85460     9.90950     5.54545     0.92828     0.56858     0.01462     0.02766
    160     0.04016     0.10218     0.99491     0.85162     9.95475     5.54545     0.93379     0.57347     0.01370     0.02766
    168     0.03844     0.10226     0.99629     0.85124    10.00000     5.54545     0.94216     0.58052     0.01343     0.02766
    170     0.03825     0.10230     0.99638     0.85073    10.00000     5.54545     0.94270     0.57482     0.01343     0.02766
    180     0.03692     0.10272     0.99692     0.84963    10.00000     5.54545     0.95202     0.56858     0.01304     0.02740
    190     0.03550     0.10347     0.99737     0.84725    10.00000     5.63636     0.95415     0.56098     0.01225     0.02792
    200     0.03411     0.10396     0.99790     0.84729    10.00000     5.54545     0.95754     0.56560     0.01159     0.02819
    210     0.03336     0.10460     0.99802     0.84530    10.00000     5.45455     0.95821     0.55760     0.01093     0.02845
    220     0.03271     0.10506     0.99813     0.84470    10.00000     5.63636     0.95781     0.54916     0.01053     0.02845
    230     0.03151     0.10584     0.99849     0.84299    10.00000     5.36364     0.96414     0.54509     0.00988     0.02845
    240     0.03061     0.10605     0.99869     0.84222    10.00000     5.45455     0.96591     0.54482     0.00935     0.02819
    250     0.02908     0.10717     0.99907     0.83993    10.00000     5.45455     0.97001     0.54373     0.00869     0.02871
    260     0.02836     0.10776     0.99923     0.83773    10.00000     5.45455     0.97559     0.53491     0.00843     0.02898
    270     0.02774     0.10817     0.99934     0.83607    10.00000     5.36364     0.97884     0.53424     0.00790     0.02898
    280     0.02714     0.10888     0.99939     0.83438    10.00000     5.18182     0.97993     0.52898     0.00764     0.02924
    290     0.02659     0.10929     0.99944     0.83468    10.00000     5.27273     0.98033     0.52910     0.00711     0.02898
    300     0.02576     0.10997     0.99956     0.83231    10.00000     5.27273     0.98562     0.53221     0.00658     0.02898
    310     0.02469     0.11043     0.99964     0.83305    10.00000     5.45455     0.98698     0.53900     0.00645     0.02950
    320     0.02415     0.11085     0.99970     0.83221    10.00000     5.41818     0.98712     0.54144     0.00632     0.02924
    330     0.02388     0.11128     0.99972     0.83145    10.00000     5.45455     0.98712     0.53235     0.00606     0.02924
    340     0.02327     0.11182     0.99979     0.83076    10.00000     5.36364     0.98793     0.53019     0.00593     0.02977
    350     0.02267     0.11229     0.99983     0.82939    10.00000     5.36364     0.98924     0.52843     0.00593     0.02977
    360     0.02206     0.11278     0.99985     0.82835    10.00000     5.18182     0.99086     0.53223     0.00579     0.02871
    370     0.02144     0.11330     0.99989     0.82832    10.00000     5.27273     0.99335     0.53588     0.00566     0.02924
    379     0.02102     0.11369     0.99990     0.82631    10.00000     5.18182     0.99430     0.52515     0.00579     0.02924
    380     0.02089     0.11374     0.99990     0.82618    10.00000     5.18182     0.99430     0.52773     0.00566     0.02898
    390     0.02079     0.11391     0.99990     0.82534    10.00000     5.27273     0.99430     0.52489     0.00566     0.02898
    395     0.02065     0.11414     0.99990     0.82415    10.00000     5.27273     0.99457     0.52462     0.00553     0.02924
    396     0.02053     0.11422     0.99990     0.82428    10.00000     5.27273     0.99417     0.52774     0.00540     0.02898
    400     0.02046     0.11433     0.99990     0.82402    10.00000     5.27273     0.99430     0.52477     0.00540     0.02898


 =========================================
 Variable Importance for the 61-tree Model
 =========================================

                   Abs     Rel

 CHILDYRS    100.00000  100.00 |***********|
 AGE          42.07834   42.08 |*****      |
 EDUC_MOM     38.70708   38.71 |*****      |
 AGE_MOM      34.23281   34.23 |****       |
 INCOME       30.44334   30.44 |****       |
 OTH_CHLD     27.98660   27.99 |****       |
 ILLEGIT      24.40604   24.41 |***        |
 RACE_MOM     14.07415   14.07 |**         |
 PNCLATE      10.52911   10.53 |**         |
 LBW           7.37271    7.37 |**         |


 Learn Sample Misclassification by Target Class
 For The 61-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 7373.00      7370.00         3.00       0.0004
 1                  221.00        85.00       136.00       0.6154


 Test Sample Misclassification by Target Class
 For The 61-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 3686.00      3675.00        11.00       0.0030
 1                  110.00        18.00        92.00       0.8364

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                2.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               2.000 sec ( 0.00 hrs, 100.00%)
    Core model:         2.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             0.000 sec ( 0.00 hrs,   0.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000000 hrs  0.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 775 kb , 76% compression

 Grove file created containing:
      1 TreeNet

