
 The "USE "../Data/Classification/8.ex.InjuryDeath/SAMPL" command: 00:00:00

 Model (target and predictors) reset: ANY_INJ_DTH

 The KEEP list has 10 variables.

 Salford Predictive Modeler(R) software suite: TreeNet(R) version 8.3.0.002

 The data cache will be established with this analysis.


 USE file Records Read: 7594
 Records Kept in Learning sample: 7594

 Note: data partitioning may differ from expected due to
 missing data, particularly in the target.


 Error file Records Read: 3796
 Records Kept in Test sample: 3796

 Discrete         N Levels
 Variable         in Model
 -------------------------
 ANY_INJ_DTH             2

 ======================
 Target Frequency Table
 ======================

 Variable: ANY_INJ_DTH
 N Classes: 2
 Data Value                   N      %            Wgt Count      %
 -----------------------------------------------------------------
 0               L         7373  97.09              7373.00  97.09
                 T        (3686  97.10)            (3686.00  97.10)
 1               L          221   2.91               221.00   2.91
                 T         (110   2.90)             (110.00   2.90)
 -----------------------------------------------------------------
 Totals
 0                        11059  97.09             11059.00  97.09
 1                          331   2.91               331.00   2.91
 -----------------------------------------------------------------
 Total                    11390                    11390.00
 Total Learn               7594                     7594.00
 Total Test                3796                     3796.00


 ===============
 TreeNet Results
 ===============


 Unit Class Weights

 TreeNet is using CXE optimality criterion.

 Loss Function: LOGIT

           ---------AveLL---------   ---------CLASS---------
 N Trees        Learn         Test        Learn         Test        Fract    Test Profile
 ----------------------------------------------------------------------------------------
      1       0.11233      0.11767      0.02910      0.02898      1.00000 |                                               *
      2       0.10457      0.11235      0.02910      0.02898      1.00000 |                                             *
      3       0.09893      0.10957      0.02910      0.02898      1.00000 |                                            *
      4       0.09396      0.10734      0.02910      0.02898      1.00000 |                                           *
      5       0.09005      0.10556      0.02910      0.02898      1.00000 |                                          *
      6       0.08677      0.10466      0.02884      0.02898      1.00000 |                                          *
      7       0.08417      0.10385      0.02884      0.02871      1.00000 |                                         *
      8       0.08179      0.10328      0.02831      0.02898      1.00000 |                                         *
      9       0.07942      0.10273      0.02765      0.02871      1.00000 |                                         *
     10       0.07748      0.10207      0.02779      0.02845      1.00000 |                                         *
     11       0.07583      0.10155      0.02686      0.02819      1.00000 |                                        *
     12       0.07427      0.10117      0.02647      0.02845      1.00000 |                                        *
     13       0.07286      0.10099      0.02620      0.02819      1.00000 |                                        *
     14       0.07140      0.10106      0.02555      0.02871      1.00000 |                                        *
     15       0.06999      0.10111      0.02541      0.02819      1.00000 |                                        *
     16       0.06923      0.10083      0.02515      0.02819      1.00000 |                                        *
     17       0.06755      0.10112      0.02502      0.02845      1.00000 |                                        *
     18       0.06600      0.10121      0.02489      0.02871      1.00000 |                                        *
     19       0.06525      0.10123      0.02436      0.02898      1.00000 |                                        *
     20       0.06447      0.10115      0.02436      0.02871      1.00000 |                                        *
     30       0.06040      0.10189      0.02265      0.02871      1.00000 |                                         *
     40       0.05626      0.10261      0.02081      0.02950      1.00000 |                                         *
     50       0.05485      0.10276      0.01949      0.02977      1.00000 |                                         *
     60       0.05312      0.10330      0.01844      0.03030      1.00000 |                                         *
     70       0.05206      0.10363      0.01791      0.03030      1.00000 |                                         *
     80       0.05102      0.10396      0.01751      0.03030      1.00000 |                                         *
     90       0.04865      0.10465      0.01686      0.03056      1.00000 |                                          *
    100       0.04761      0.10482      0.01659      0.03056      1.00000 |                                          *
    110       0.04657      0.10531      0.01607      0.03003      1.00000 |                                          *
    120       0.04453      0.10623      0.01514      0.03003      1.00000 |                                          *
    130       0.04363      0.10677      0.01501      0.02924      1.00000 |                                           *
    140       0.04240      0.10762      0.01409      0.03030      1.00000 |                                           *
    150       0.04005      0.10853      0.01317      0.03030      1.00000 |                                           *
    160       0.03925      0.10912      0.01198      0.03003      1.00000 |                                            *
    170       0.03793      0.10992      0.01172      0.03030      1.00000 |                                            *
    180       0.03622      0.11098      0.01106      0.03109      1.00000 |                                            *
    190       0.03545      0.11166      0.01106      0.03135      1.00000 |                                             *
    200       0.03515      0.11211      0.01106      0.03135      1.00000 |                                             *
    210       0.03481      0.11239      0.01106      0.03109      1.00000 |                                             *
    220       0.03311      0.11291      0.00988      0.03109      1.00000 |                                             *
    230       0.03195      0.11388      0.00935      0.03135      1.00000 |                                             *
    240       0.03096      0.11469      0.00909      0.03056      1.00000 |                                              *
    250       0.03015      0.11546      0.00909      0.03135      1.00000 |                                              *
    260       0.02911      0.11651      0.00869      0.03109      1.00000 |                                               *
    270       0.02840      0.11752      0.00830      0.03082      1.00000 |                                               *
    280       0.02748      0.11858      0.00790      0.03135      1.00000 |                                               *
    290       0.02594      0.11950      0.00698      0.03109      1.00000 |                                               *
    300       0.02446      0.12125      0.00632      0.03135      1.00000 |                                               *
    310       0.02395      0.12175      0.00606      0.03188      1.00000 |                                               *
    320       0.02268      0.12288      0.00566      0.03135      1.00000 |                                               *
    330       0.02134      0.12372      0.00487      0.03135      1.00000 |                                               *
    340       0.02050      0.12498      0.00474      0.03188      1.00000 |                                               *
    350       0.01983      0.12561      0.00448      0.03214      1.00000 |                                               *
    360       0.01934      0.12599      0.00421      0.03214      1.00000 |                                               *
    370       0.01872      0.12652      0.00395      0.03188      1.00000 |                                               *
    380       0.01863      0.12702      0.00408      0.03214      1.00000 |                                               *
    390       0.01803      0.12739      0.00395      0.03240      1.00000 |                                               *
    400       0.01766      0.12812      0.00395      0.03240      1.00000 |                                               *

 Core TN model building:          1.000 sec ( 0.00 hrs)



 ----- Presence -----    ---- Top Depth -----     Depth
 NTrees  First   Last    Min    Max       Avg     Score  Predictor
 ------------------------------------------------------------------
    400      1    400      1      3      1.17      6.83  CHILDYRS
    256      1    400      1      7      4.35      2.34  AGE
    227      1    400      2      7      5.00      1.70  AGE_MOM
    203      1    400      1      7      4.93      1.56  INCOME
    193      1    400      1      7      4.79      1.55  EDUC_MOM
    174      1    400      1      7      5.09      1.27  OTH_CHLD
    132      1    397      1      7      5.45      0.84  ILLEGIT
     85      1    389      1      7      4.98      0.64  RACE_MOM
     57      1    389      1      7      5.28      0.39  LBW
     55      4    385      1      7      6.02      0.27  PNCLATE

 Note: Top Depth is conditional on predictor appearing in tree.
       Depth Score == 0.0 for a predictor not appearing in tree.
       Depth == 1 for the root node.

 Characterization of TN tree dimensionality:
    Smallest:     8 terminal nodes
    Largest :    75 terminal nodes
    Average :     21.93000 terminal nodes

 Reconciling 7594 Learn sample scores across 5 selected models,
 the largest having 16 trees, to compute gains and PS tables.

 Reconciling 3796 Test sample scores across 5 selected models,
 the largest having 16 trees, to compute gains and PS tables.


 ========================
 TreeNet Model Dimensions
 ========================

 N Trees
       Total: 400
     Optimal: 16

 Target: ANY_INJ_DTH
 Focus Class: 1

                     N     Weighted
 N Learn Obs:     7594      7594.00
 N Test  Obs:     3796      3796.00
 Learn Rate :    0.1000000

 Storage requirements: 17144 tree / 1 categorical splits

 Mean time per tree: 00:00:00.00
 Logistic Model.


 ==========================
 Learn and Test Performance
 ==========================

 Optimality Criterion      N-trees      Test/CV
 ----------------------------------------------
                AveLL           16      0.10083
                  ROC            5      0.87886
                 Lift            4      6.00000
              KS-stat            9      0.60413
          Class.Error           11      0.02819

              AveLL       AveLL         ROC         ROC        Lift        Lift     KS-stat     KS-stat Class.Error Class.Error
  Trees       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV       Learn     Test/CV
 ------------------------------------------------------------------------------------------------------------------------------
      1     0.11233     0.11767     0.92862     0.82246     7.35893     5.36970     0.70705     0.56355     0.02910     0.02898
      4     0.09396     0.10734     0.94413     0.87861     8.23529     6.00000     0.76124     0.58271     0.02910     0.02898
      5     0.09005     0.10556     0.94604     0.87886     8.37104     6.00000     0.76433     0.59761     0.02910     0.02898
      9     0.07942     0.10273     0.95761     0.87574     8.68778     5.87273     0.80078     0.60413     0.02765     0.02871
     10     0.07748     0.10207     0.95965     0.87487     8.82353     5.90909     0.81793     0.59694     0.02779     0.02845
     11     0.07583     0.10155     0.96089     0.87779     8.91403     5.81818     0.82250     0.59748     0.02686     0.02819
     16     0.06923     0.10083     0.96630     0.87348     9.04977     5.90909     0.83282     0.58839     0.02515     0.02819
     20     0.06447     0.10115     0.97291     0.87221     9.18552     5.81818     0.85657     0.57509     0.02436     0.02871
     30     0.06040     0.10189     0.97614     0.86652     9.23077     5.81818     0.85367     0.55867     0.02265     0.02871
     40     0.05626     0.10261     0.98068     0.86485     9.32127     5.72727     0.87032     0.55193     0.02081     0.02950
     50     0.05485     0.10276     0.98206     0.86581     9.45701     5.72727     0.88154     0.54810     0.01949     0.02977
     60     0.05312     0.10330     0.98342     0.86584     9.41176     5.72727     0.89154     0.54610     0.01844     0.03030
     70     0.05206     0.10363     0.98427     0.86457     9.45701     5.72727     0.89004     0.53886     0.01791     0.03030
     80     0.05102     0.10396     0.98480     0.86392     9.45701     5.72727     0.88977     0.53428     0.01751     0.03030
     90     0.04865     0.10465     0.98835     0.86036     9.59276     5.81818     0.90338     0.53029     0.01686     0.03056
    100     0.04761     0.10482     0.98910     0.86006     9.59276     5.90909     0.90506     0.53737     0.01659     0.03056
    110     0.04657     0.10531     0.98990     0.85896     9.63801     5.90909     0.91384     0.53846     0.01607     0.03003
    120     0.04453     0.10623     0.99118     0.85680     9.63801     5.72727     0.91818     0.53927     0.01514     0.03003
    130     0.04363     0.10677     0.99189     0.85575     9.68326     5.72727     0.92216     0.53927     0.01501     0.02924
    140     0.04240     0.10762     0.99248     0.85477     9.68326     5.72727     0.92325     0.54619     0.01409     0.03030
    150     0.04005     0.10853     0.99398     0.85216     9.81900     5.69091     0.92708     0.53927     0.01317     0.03030
    160     0.03925     0.10912     0.99425     0.85067     9.81900     5.63636     0.93007     0.54429     0.01198     0.03003
    170     0.03793     0.10992     0.99480     0.84781     9.81900     5.54545     0.93690     0.54619     0.01172     0.03030
    180     0.03622     0.11098     0.99574     0.84544     9.81900     5.50909     0.93825     0.53154     0.01106     0.03109
    190     0.03545     0.11166     0.99630     0.84427     9.86425     5.45455     0.93907     0.53425     0.01106     0.03135
    200     0.03515     0.11211     0.99636     0.84310     9.86425     5.45455     0.93893     0.53480     0.01106     0.03135
    210     0.03481     0.11239     0.99650     0.84278     9.86425     5.45455     0.94798     0.52787     0.01106     0.03109
    220     0.03311     0.11291     0.99698     0.84134     9.86425     5.45455     0.95097     0.52014     0.00988     0.03109
    230     0.03195     0.11388     0.99748     0.83854     9.90950     5.54545     0.95522     0.50237     0.00935     0.03135
    240     0.03096     0.11469     0.99775     0.83712     9.90950     5.36364     0.95875     0.50983     0.00909     0.03056
    250     0.03015     0.11546     0.99793     0.83566     9.90950     5.09091     0.96088     0.50305     0.00909     0.03135
    260     0.02911     0.11651     0.99810     0.83333     9.95475     5.00000     0.96227     0.50291     0.00869     0.03109
    270     0.02840     0.11752     0.99821     0.83132     9.95475     5.00000     0.96295     0.50779     0.00830     0.03082
    280     0.02748     0.11858     0.99834     0.82919     9.95475     5.00000     0.96336     0.50264     0.00790     0.03135
    290     0.02594     0.11950     0.99869     0.82709     9.95475     4.81818     0.97005     0.50983     0.00698     0.03109
    300     0.02446     0.12125     0.99906     0.82319     9.95475     5.00000     0.97620     0.50183     0.00632     0.03135
    310     0.02395     0.12175     0.99910     0.82358     9.95475     5.00000     0.97607     0.51485     0.00606     0.03188
    320     0.02268     0.12288     0.99924     0.82091    10.00000     4.81818     0.97797     0.50589     0.00566     0.03135
    330     0.02134     0.12372     0.99973     0.82098    10.00000     4.72727     0.98951     0.50440     0.00487     0.03135
    340     0.02050     0.12498     0.99983     0.81969    10.00000     4.72727     0.99032     0.50711     0.00474     0.03188
    350     0.01983     0.12561     0.99988     0.82088    10.00000     4.81818     0.99263     0.50670     0.00448     0.03214
    360     0.01934     0.12599     0.99989     0.82052    10.00000     4.81818     0.99263     0.50833     0.00421     0.03214
    363     0.01896     0.12604     0.99990     0.82100    10.00000     4.81818     0.99263     0.50915     0.00395     0.03188
    370     0.01872     0.12652     0.99991     0.82141    10.00000     4.72727     0.99263     0.51104     0.00395     0.03188
    380     0.01863     0.12702     0.99991     0.81995    10.00000     4.72727     0.99276     0.51132     0.00408     0.03214
    390     0.01803     0.12739     0.99992     0.81991    10.00000     4.81818     0.99276     0.50887     0.00395     0.03240
    397     0.01772     0.12795     0.99995     0.81975    10.00000     4.81818     0.99385     0.51077     0.00395     0.03240
    399     0.01770     0.12801     0.99995     0.81932    10.00000     4.81818     0.99385     0.51050     0.00395     0.03240
    400     0.01766     0.12812     0.99995     0.81909    10.00000     4.81818     0.99385     0.51023     0.00395     0.03240


 =========================================
 Variable Importance for the 16-tree Model
 =========================================

                   Abs     Rel

 CHILDYRS    100.00000  100.00 |***********|
 EDUC_MOM     43.12269   43.12 |*****      |
 AGE          40.64949   40.65 |*****      |
 INCOME       33.24392   33.24 |****       |
 AGE_MOM      31.75590   31.76 |****       |
 OTH_CHLD     29.99847   30.00 |****       |
 ILLEGIT      25.24369   25.24 |***        |
 RACE_MOM     17.18000   17.18 |***        |
 LBW          11.71387   11.71 |**         |
 PNCLATE       7.39707    7.40 |**         |


 Learn Sample Misclassification by Target Class
 For The 16-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 7373.00      7366.00         7.00       0.0009
 1                  221.00        37.00       184.00       0.8326


 Test Sample Misclassification by Target Class
 For The 16-Tree Model
 Accumulation: True Class by Predicted Class
 All Counts are Weighted

 Class             N Total    N Correct   N Misclass  Prop Misclass
 0                 3686.00      3679.00         7.00       0.0019
 1                  110.00        10.00       100.00       0.9091

 Plot queue is empty.

 Smoothed plots queue is empty.

 MARTDO:                2.000 sec ( 0.00 hrs)
  LOADDATA 1:           0.000 sec ( 0.00 hrs,   0.00%)
  MARTGO:               1.000 sec ( 0.00 hrs,  50.00%)
    Core model:         1.000 sec ( 0.00 hrs, 100.00%)
  MARTPRED:             0.000 sec ( 0.00 hrs,   0.00%)
  PLOTS/INTER:          0.000 sec ( 0.00 hrs,   0.00%)

  Reconciling (LEARN):       0.000000 hrs  0.00%

  Reconciling (TEST ):       0.000000 hrs  0.00%


 Grove file created: /home/jries/projects/SPM_vs_XGBOOST/Scripts/mart_model.grv: 806 kb , 76% compression

 Grove file created containing:
      1 TreeNet

